Parallel programming with python=Jan Palach;Note=Erxin

# Cover
# Copyright
# Credits
# About the Author
# Acknowledgement
# Table of Contents
# Preface
- what this book covers
- who this book is for, for beginner and intermediate python developers
- Conventions
- Reader feedback
- Customer support
- Downloading the example code
- Errata
- Piracy
- Questions

# contextualizing parallel, concurrent, and distributed programming
- moore law 
http://www.mooreslaw.org/
- why use parallel programming
without blocking UI
- exploring common forms of parallelization
concurrent programming is an abstraction from parallel programming

distributed programming aims at the possibility of sharing the processing by exchanging data through messages between machines(nodes) of computing which are physically separated
- advantage of distributed programming
    + fault-tolerance
    + horizontal scalability
    + cloud computing
    
- communication in parallel programming, share state and message, shared state control by data access tech such as mutex 
- understanding message passing, is used when we aim to avoid data access control and synchronizing problems originating from shared state
    + erlang use message dispatching tech
    + absence of data access concurrence
    + messages can be exchange locally(various processes) or in distributed environments 
    
- deadlock, two or more worker keep indefinitely waiting for freeing of a resource, process wait for condition to free their task, but this condition will never occur
- starvation, unfair raking of one or more processes that take much more time to run a task
- race condition, a process depends on a sequence of facts, this sequence is broken due to lack of synchronizing mechanisms
the modification and read operation are lack synchronization will lead to race condition, in this case synchronization operation such as mutex, condition, lock are essential

- discovering python's parallel programming tools, base on python 3.x
    + threading module, base on thread
    + multiprocessing module, base on process
    + the parallel python module
    http://parallelpython.com
    
        * automatic detection of the optimal configuration
        * a number of worker processes can be changed during runtime
        * dynamic load balance
        * fault tolerance
        * auto-discovery of computational resources
        
    + celery, a distributed task queue, it makes uses of least three different approach to run tasks in concurrent form
        * multiprocessing
        * Eventlet
        * Gevent
    http://celeryproject.org
    
- taking care of python GIL,is sued in standard python, known as CPython, to avoid bytecodes that are executed simultaneously by different threads
 
pypy team is working on an STM implementation in order to remove GIL from python
http://pypy.org/tmdonate.html
    
    
# design parallel algorithms
- the divide and conquer technique
    + sort algorithms such as merge sort and quick sort
    
    + merge sort, split the inputs-> sort separately -> merge the mid result -> merge the final result
    
- using data decomposition
- decomposing tasks with pipeline
    + identify independent tasks
    + identify task

- load balance


# identify a parallelizable problem
- obtaining the highest fibonacci value for multiple inputs
- memchached
http://memcached.org/
- crawling the web
    + group all the input URIs in a data structure
    + associate data URLs with tasks
    + dispatch the tasks for executing in parallel workers
    + the result from the previous stage must be passed to the next stage


# using the threading and concurrent.futures modules
- thread, is different execution lines in a process, threads belong to the same process share the same memory space
- advantages and disadvantages of using threads
    + advantages
        * speed of communication of the threads in the same process, data, location is fast
        * creation threads is less costly than process
        * make the best use of data locality by optimizing memory access through the processor cache memory
    + disadvantages
        * difficult to solve errors for inexperienced developers
        * it's hard to migrate to a distributed architecture
        
- different kinds of threads, two types of threads, kernel and user
    + kernel are the threads created and managed by the operating system, exchange context, scheduling and concluding all by kernel
    
        * one kernel thread is referenced to on process
        * kernel threads can run on different cpus
        * creation are expensive
        * platform dependent
    
    + user thread manage by package developer
        * low cost for creation and synchronization
        * platform independent
        * all user threads inside a process are related to only one kernel thread
        * user threads can't run on different cpus

- defining the states of a thread
    + create
    + execution
    + ready
    + blocked
    + concluded, free resources are to be used in a execution and end life span of the thread

- choosing between threading and _thread
    + _thread is a lower level implementing, for implement own thread pool and cuddling with locks
    
- using threading module with condition, use condition object with with state
with conditionObject:
    pass
- crawling the web using the concurrent futures module
concurrent.futures module contain the ThreadPoolExecutor object

with concurrent.futures.ThreadPoolExecutor(max_workers=3) as glt:
    pass
    
also contain ProcessPoolExecutor object to help step side GIL, global interpreter lock
import concurrent.futures
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

def is_prime(n):
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

if __name__ == '__main__':
    main()

future objects, are created by Executor.submit() method
    
    
# using multiprocessing and ProcessPoolExecutor
- understanding the concept of a process
- process module
    + processes have associated information and resources that allow their manipulation and control
    + operating system has a structure called the Process Control block(PCB)
        * process id
        * program counter, this contains the address of the next program instruction to be executed
        * I/O information
        * Memory allocation
        * CPU scheduling
        * Priority
        * Current state
        * CPU registry, stores stack pointers and other information
        
- defining the states of a process
    + running
    + ready
    + waiting
    
- implementing multiprocessing communication
    + using muliprocessing.Pipe
        
- using ProcessPoolExecutor craw the web
with concurrent.futures.ProcessPoolExecutor(
    max_workers=number_of_cpus) as crawler_link_processes:
        future_tasks = {crawler_link_processes.submit(crawl_task,url, html_link_regex):
            url for url in result_dict.keys()}
        for future in concurrent.futures.as_completed(future_tasks):
            result_dict[future.result()[0]] = future.result()[1]
        
    
# utilizing parallel python
- present named pipes and how to use  Parallel python(PP) to perform parallel tasks with processes
- understanding interprocess communication
    + shared memory
    + message queues
    + pipes
    + sockets 
    + remote procedure call

- process communicate between unrelated processes
    + exploring named pipes
    + within the POSIX system such as linux, 
    file descriptor are mechanisms that allow the user program to access files for read/write operations
    + named pipes are nothing but mechanisms that allow IPC communication through the use of file descripters, FIFO
    regular pipes are created in memory
    named pipes make use of the file descriptors and special files in a file system
about the file descriptor
http://publib.boulder.ibm.com/infocenter/pseries/v5r3/index.jsp?topic=/com.ibm.aix.genprogc/doc/genprogc/fdescript.htm
    
- using named pipes with python
    + create in the producer process

    import os

    named_pipe = 'my_pipe'
    if not os.path.exists(named_pipe):
        os.mkfifo(named_pipe)
        
    mkfifo creats a special file that implements a FIFO mechanism for the writing and reading of messages

    def write_message(input_pipe, message):
        fd = os.open(input_pipe, os.O_WRONLY)
        os.write(fd, (message % str(os.getpid())))
        os.close(fd)
    
    + create in reader process
        
    named_pipe = 'my_pipe'
    def read_message(input_type):
        fd = os.open(input_pipe, os_RONLY)
        message = ("I pid [%d] received a message => %s" % (os.getpid(), os.read(fd, 22))
        os.close(fd)
        return message
        
- discovering PP
    + source
    http://www.parallelpython.com/
    
    + features
        * automatic detection of number of processors
        * many processors allocated can be changed at runtime
        * load balance at runtime
        * auto-discovery resources throughout the network
        
    + using PP to calculate the fibonacci series term on SMP architecture
    job_dispatcher = pp.Server(ncpus=1, ppservers=ppservers, socket_timeout=60000)
    
    job_dispatcher.submit(crawl_task, (url,), modules=('os', 're', 'requests',), callback=aggregate_results)
    
    job_dispatcher.wait()
    
    init the ppserver.py utility in the remote machines, ppserver.py -a -d
    the -a, is automatic discovery, -d show the information on the activities of the server are performing
    

# distributing tasks with celery
- distribute tasks among different machines in a network by using the Celery framework
http://celery.readthedocs.org
- why use celery
    + distributes tasks in a transparent way
    + changes in a simple way the concurrence of workers through setup processes, threads, Gevent, Eventlet
    + supports synchronous, asynchronous, periodic and scheduled tasks
    + re-executes task
    
- understanding celery's architecture
    + celery has an architecture based on pluggable components
    + message transports(broker)
    contain several task queues
                                +-------------------------+
    client A -sending message-> |message transport(broker)|
                                |                         |-> worker A
    client B -sending message-> |task queue X             |
           A                    |task queue Y             |-> worker B
           |                    |                         |
           |                    +-------------------------+    |
           |                                                   |
           +--------------------backend results <--------------+
    
- working with tasks
@app.task
def hello_world():
    return 'hello'
    
any callable can be a task
    
to dispatch a task we should make use of some of the following methods of task
    + delay(arg, kwargs=value), shortcut to call the apply_async
    + apply_async((arg,),{'kwarg':value}) allows the setting up of a series of interesting parameters for the execution of the task

- discovering message transport(broker)
we get to send and receive message and communicate with worker through the broker. not all Celery mechanisms are implemented the most complete in terms are RabbitMQ and Redis

- understanding worker
    + concurrency mode
    + remote control, we can send message directly to a specific worker or a list of them through a high priority queue
    + revoking tasks, we can instruct one or more workers to ignore the execution of one or more tasks
    
- result backends, storing the status and result of the task to return to the client application result backend supported
    + RabbitMQ
    + Redis
    + MongoDB
    + Memcached
   
- setting up the environment
    + setting up the client machine
    $ pyvenv celery_env
    
    this will create a python virtual environment for celery
    
    activate the virtual environment by the command from root of celery_env
    $ source bin/activate

    use pip to install celery
    $ pip install celery
    
    install Redis in celery
    
    $ pip install celery[redis]
    
    + setting up the server
    
    $ sudo apt-get install redis-server
    
    start the server
    $ redis-server
    
    + dispatching a simple task
    from match import sqrt
    from celery import Celery
    
    app = Celery('tasks', broker='redis://192.168.25.21:6379/0')
    
    app.config.CELERY_RESULT_BACKEND = 'redis://192.168.25.21:6379/0'
    
    @app.task
    def squre_root(value):
        return sqrt(value)
        
    copy tasks.py module inside the directory and run it by
    $ celery -A tasks worker --loglevel=INFO
    
    
# doing things asynchronously
- understanding blocking, nonblocking and asynchronous operations
- understanding event loop
    + polling functions, in terms of linux
    
    select(), limitation in the number of resource descriptors to be monitored
    
    poll(), is an enhancement in response to select() reuses entry data in its call
    
    epool(), powerful implementation to linux and has the attractive feature of constant complexity

- using event loops
    + make use of poller objects
    + loops of events in general, make use of callback functions, application implement event loop in python 
        * tornado web server
        * twisted, popular framework of 
        https://twistedmatrix.com/trac/
        * asyncio, offer a event loop
        https://doc.python.org/3.4/library/asyncio.html
        * eventlet, implements an event loop based on libevent
        https://pypi.python.org/pypi/eventlet
        * Gevent, provides an event loop based on libev
        http://www.gevent.org/
        
- using asyncio, define asyncio as a module that came to reboot asynchronous programming in python
    + Event loop, allows event loop per process
    + Coroutines, official documentation of asyncio, 'a coroutine is a generator that follows certain conventions', it can be suspended during execution to wait for external processing(some routine in I/O) and restart when the external processing is done
    
    + Futures, defines its own future, represent a processing that has still not been accomplished
    + tasks, is a subclass of asyncio.Future to encapsulate and manage coroutines
    
- understanding coroutines and futures.Future objects
    + Coroutine is initialized and asyncio.Future object is instanced internally or passed as an argument to coroutine
    + when use yeild from, the coroutine is then suspended to wait for computing evoked in yield from waits for <coroutine or asyncio.Future or asyncio.Task construction
    + when evoked computing in yield from ends, the coroutine executes the set_result(<result>) method
    
- using coroutine and asyncio.Future
import asyncio

@asyncio.coroutine
def sleep_coroutine(f):
    yeild from asyncio.sleep(2)
    f.set_result('done')
    
if __name__ == '__main__':
    future = asyncio.Future()
    loop = asyncio.get_event_loop()
    loop.run_until_complete(sleep_coroutine(future))
    
f is an instance of asyncio.Future, the resumption is done by f.set_result, all the coroutines to be resumed need to wait for asyncio.Future to execute the set_result method
        
- using asyncio.Task

import asyncio
@asyncio.coroutine
def sleep_coro(name, seconds=1):
    print('[%s] coroutine will sleep for %d second(s)', % (name, seconds))
    yield from asyncio.sleep(seconds)
    print('%s done' % name)
        
if __name__ == '__main__':
    tasks = [asyncio.Task(sleep_coro('TaskA', 10),
             asyncio.Task(sleep_coro('TaskB'),
             asyncio.Task(sleep_coro('TaskC')]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.gather(*tasks))
        
- using an incompatible library with asyncio, because asyncio is new into python library some of the library is not compatible with, if replace the asyncio.sleep() to time.sleep may trigger exception in the previous example, in this case we need Future object

def sleep_func(seconds):
    f = asyncio.Future()
    time.sleep(seconds)
    f.set_result('Future done')
    return f
        
in this case all the coroutine will run serial condition. to avoid this need to use ThreadPoolExecutor for I/O bound, if CPU-bound use ProcessPoolExecutor

@ asyncio.coroutine
def sleep_coro(name, loop, seconds=1):
    future = loop.run_in_executor(None, time.sleep, seconds)
    yield from future
        
if __name_- == '__main__':
    loop = asyncio.get_event_loop()
    tasks = [asyncio.Task(sleep_coro('Task-A', loop, 10)),,
             asyncio.Task(sleep_coro('Task-B', loop)),
             asyncio.Task(sleep_coro('Task-C', loop)),]
        
    loop.run_until_complete(asyncio.gather(*tasks))
    loop.close()
    

        
        
        
        
        
        
        
        