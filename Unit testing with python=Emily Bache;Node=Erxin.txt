Unit testing with python=Emily Bache;Node=Erxin

# course summary
- vocabulary
- why and when
- an alternative to unittest pytest
- doctest
- test doubles
- assessing test coverage
- maintainable unit tests

# using unittest
- review of fundamentals
    + system under test
    + a unit test checks the behaviour of an element of code
        * a method or function
        * a module or class
    + an automated test
        * is designed by a human
        * runs without intervention
        * reports results unambiguously as 'pass' as 'fail'
    + it's not a unit test if it uses
        * the file system
        * a database
        * the network
- a first test case, exercise phone numbers
    + given a list of names and phone numbers, make a phonebook that allows you to look up numbers by name
    + determine if a given phonebook is consistent
        * in a consistent phone list no number is prefix of another
        * for example
        Bob 91125426
        Alice 97 625 992
        Emergency 911
        
        Bob and Emergency are inconsistent

    + write and run the unit test
    python -m unittest 
    this will run the unittest mode and search the current directory to find all the unittest and run

- test runner is run the test and give the report
- eclipse have a pyunit test panel
- a test case using assertRaises, expanation of 'test suit'
write a unittest with a expected error

class PhonebookTest(unittest.TestCase):
    def test_missing_entry_raises_KeyError(self):
        pb = Phonebook()
        with self.assertRaises(KeyError):
            phonebook.lookup('missing_key')

- run a single test method
python -m unittest -q package.TestCaseClass.testMethod -v

v, is use to display verbose message

- test suite
test case -> test suite -> test runner

- skip a test case, marking it work in progress
@unittest.skip("message")
def test_method(...)

- add setUp and tearDown method for each test case class
def setUp(self):
    pass
    
def tearDown(self):
    pass
test fixture is used to configuration each test case

if setup is failed, tear down will not run

- re-introduce the skipped test case, get it to pass
remove the skip decorator to add in the test case
- test case design
    + name, should give a good indication for test case
    + arrange
    + act 
    + assert
    + tear up the whole functional test into unit function test, such as 
        * test the phone book is consistent function in one test case is not a right one
        * break down the function into several condition and mark the condition with test name is better than test whole function just in one test case
        normal entries is consistent
        duplicate entries is consistent
        number that prefix one another is consistent

- the three parts of a test case
    + arrange, set up the object to be tested & collaborators
    + act, excise functionality on the object
    + assert, make claims about the object & its collaborators
    + cleanup release resources, store to original state

- unit test documentation, unittest documentation in python website


# Why and When Should you write unit test 
- module overview 
    + what is unit testing ofr 
    + unit testing in your personal development process 
        * test first 
        * test last 
        * test driven 
        
    + continuous integration 

- what is unit testing for 
    + understand what to build
    + document the units 
    + design the units 
    + regression protection 

- understanding 
    + understand what to build 
    + collaborate with people in other roles to understand what's needed 
        * business analyst, product owner 
        * tester 
        * interaction designer 
        * lead developer 
        
import SeatFinder 
        
class SeatFinderTest(unittest.TestCase):
    def test_prefer_near_the_front(self):
        finder = SeatFinder(available_seats={'a6', 'b6', 'c7'})
        seats = finder.find_seats(1)
        
        assert seats={'a6'}
    
    
- documenting 
    + document the units 
        * "executable specification", tests document the behavior of the code 
        * how the unit is intended to use 
        
- design, make sure decompose the problem into units that are independently testable, loose coupling 
    + design the interface separately from doing the implementation 
    
- design regression protection 
    + regression something worked 
    a unit test should fail & point out which unit failed and why 
    
- limitation of unit test, can't 
    + can't guarantee your software don't have bugs 
    + unit testing won't find integration faults
    + performance 
    + security 
    
- testing as part of your personal development process  
    + production code 
    + test suite 
    
    + automatic test process to help maintain the code 
    
- test last, avoid rewrite the test cases, risk discover testability problems and bugs late in the process, you may missing important test case 
    + design code 
    + design tests 
    
- test first, risk is rework to write the test, need refactor the test and code 
    + design code 
    + design test 
    + write code 
    
- test driven 

    + write a test 
    + refactor 
    + write a little code 
    
    make your test pass and then write another test and write the code 
    
- continuous integration 
    + unit testing in the wider development team 
    + version control system 
        * production code 
        * test suit 
        
- work from known good 
    + unexpected failures, use CI server to continuous integration server 
    http://www.martinfowler.com/articles/continuousIntegration.html
    
    + coverage metrics normally is coverage to 60% 
    
- module review 
    

# Using Pytest for Unit Testing in Python 
- module overview 
    + defining test cases with py.test 
    + interpreting test failures 
    + test fixtures 
    
- xUnit 
    + Kent Beck 
    + Erich Gamma
    + JUnit 
    JUnit was originally created by Kent Beck and Erich Gamma 
    
import unittest 
class Phonebook(unittest.TestCase):

    def setUp(self):
        self.phonebook = Phonebook()
        
    def test_add_and_lookup_entry(self):
        self.phonebook.add("Bob", "1234")
        self.assertEqual("1234", self.phonebook.lookup("Bob"))
    
    
default python unittest module doesn't follow the PEP quite well 
    
    + alternative of unittest 
        * python-nose
        
        a unittest-based testing framework for python that makes writing and running tests easier 
        
        * pytest 
        a mature full-featured python test tool 
    
    + pytest is not included by default 
    
- defining and running a simple test case by pytest, only required to name the test code file with test_*.py and the test method start with test_*

def test_add_and_lookup_entry():
    phonebook = Phpnebook()
    phonebook.add("bob", "123")
    assert "123" == phonebook.lookup("bob")[, additional information for assert when failed]
    
execute a test by 
$ python -m pytest 
    
pytest output information will include all the test method and mark the fail line 
suggest not add message for assert statement 

interpreting failure information 
- asserting the contents of collections 
- built-in helpers functions 'raises' and 'skip'

def test_missing_entry_raises_KeyError():
    phonebook = Phonebook()
    with pytest.raises(KeyError):
        phonebook.lookup("Unexist")
    
pytest.raises will check if the underline code of with raise the proper exception KeyError

def test_add_and_lookup_entry():
    pytest.skip("WIP")
    ...
    
the test method start with pytest.skip will be skipped during the test 
    
- test fixture
    + unittest 
test case -> test fixture (setup, teardown) -> system under test 

          -> test suit -> test runner 
          
    + pytest 
    
test case -> request a resource -> test fixture 

@pytest.fixture 
def phonebook():
    return Phonebook()
    
def test_add_and_lookup_entry(phonebook):
    phonebook.add("bob", "123")
    assert "123" == phonebook.lookup("bob")

the test runner will try to allocate the test phonebook fixture during a test run 
    
add finalize for test fixture     

@pytest.fixture 
def phonebook(request):
    phonebook = Phonebook()
    def cleanup_phonebook():
        phonebook.clear()
    request.addfinalizer(cleanup_phonebook)
    reutrn phonebook 

- using built-test fixture resources - 'tmpdir'
    + tmpdir is buld-in fixture of pytest
    + there are several builtin fixtures of pytest 
    
@pytest.fixture 
def phonebook(tmpdir):
    phonebook = Phonebook(tmpdir)
    reutrn phonebook 
    
a unit test should avoid use file system 
    
$ python3 -m pytest --fixtures
    
both test case and test fixture can allocate resources 

- using pytest to run 'unittest' tests 

run unittest by 
$ python -m unittest 

pytest test runner could also be used to run unittest and doctest 
$ python -m pytest 

    
# Testable documentation with Doctest 
- module outline 
    + situations when you'd use doctest 
        * checking examples in docstrings
        * regression testing 
        * tutorial documentation 
        
    + making documentation comments more truthful 
        
    + handling output that changes 

- making documentation comments more truthful 
def factorial(n):
    '''
    >>> [factorial(n) for in range(6)]
    [1,1,2,6,24,120]
    '''
    ...
    
- regression protection, also could make doc comments more truthful 
Yatzy example 
two pairs, 2+2 + 3+3 = 10

three, score 3+3 = 6 

def small_straight(dice):
    '''score the given roll in the small straight Yatzy category 
    
    Args:
        dice: a sorted list of 5 integers indicating the dice rolled 
        
    Returns:
        an integer score 
    '''
    if dice == [1,2,3,4,5]:
        return sum(dice)
        
    else:
        return 0
        
doctest will automatically import the current module. Could add the interpreter operations as docstring test

- using different test runners to execute doctests 

    + you have to give the python file directory 
    $ python -m doctest *.py 
    $ python -m doctest -v *.py 

    + use pytest will automatic find and run the doctest 
    $ python -m pytest --doctest-modules 

    but the pytest run the doc test in different way 
    + use unittest to run the doctest by a little additional works, name the file as test_all_doc_test.py
    import unittest 
    import doctest 
    import yatzy 
    
    def load_tests(loader, tests, ignore):
        test.addTests(doctests.DocTestSuite(yatzy))
        return tests 
        
    $ python -m unittest 
    
- handling failing doctests, pytest will stopped whenever a docstring test failed
- handling output that changes- dictionary and floats, sort the output before output, round the float the number before write doc test 
- testing for exceptions, including tracebacks in doctests for doctest will only check the trackback and the TypeError 

>>> trigger_some_exception()
Trackback(most recent call last):
[...] # this is optional for exception check in doctest 
TypeError:Can't convert

the ... is inform the doctest the content which you are not care about
    
- the ellipsis directive, a wildcard for matching varying output 
'''
>>> scores_in_categries([1,2,2,2]) #doctest: +ELLIPSIS
[(8, <function full_house at ...>)]

the #doctest:+ELLIPSIS is a directive for use ellipsis matching output, the pytest don't require the mark #doctest: +ELLIPSIS 

- putting doctest regression tests in a separate file 
- when to put doctest in a file, using approval testing 
    + I'll know it when I see it 
    write a test -> refactor -> write a little code 
    
    + a too long doctest may make your code bigger, if a doctest is for document the units, write it in a docstring
    if a doctest is for regerssion test, put it in a separate file & consider moving to unittest or pytest 
    
- doctest for checking tutorial documentation 
    + tutorial documentation

    
# Test Doubles :Mocks, Fakes, Stubs and Dummy object 
- what is a test double? 
    + like "stunt doubles" who stand in for actors in films 
    + class under test doesn't know it isn't talking to the real-object 
    + allow you to control what happens in your test 
    
    from http://xunitpatterns.com
    https://en.wikipedia.org/wiki/Test_Double 
    
    + Stub 
    + Fake 
    + Mock 
    + Test Spy 
    + Dummy Object 
- example using a stub 
Alarm       ----> sensor 
+ check           + sample_pressure()

want to test this method 

class Alarm(object):
    def __init__(self, sensor=None):
        self._sensor = sensor or Sensor()  #assign the _sensor if sensor is None
        self._is_alarm_on = False 
        
custom define a test sensor class 

class TestSensor(object):
    def __init__(self, pressure):
        self._pressure = pressure 
- using unittest.mock to create a stub 
import unittest
from unittest import Mock 

test_sensor = Mock(Sensor)
test_sensor.sample_pressure.return_value = 18

if you change the Sensor class an forget to change the Alarm test class the mock unittest framework will warn
 
- stub example summary 
    + stub 
    Sub has no logic or advanced behavior, only make sure generate the same interface stub class 

    + TestSensor 
    sample_pressure()
    
- Example using a Fake, fake is rather than a stub object, fake have logic and some function that works in some level but not suit for work in production 

    + try to make the class more testable 
    + such as write FileAccessWrapper to wrap the file object instead of direct access the file system during test 
    
    + common things to replace with fake 
    File, Database, WebServer 
    
- Example using a mock 
    + mock contain hard code values
    + three kinds of assert 
        * check the return value or an exception 
        * check a state change(use a public api)
        * check a method call(use a mock or spy)

    + interaction testing for example test a web service 
    MyService 
     handle(request, token)

     SSORegistery
     register(id):token 
     is_valid(token)
     unregistered(token)
     
     MyServiceTest
     test_valid_token
     test_invalid_token
     
     + three kinds of assert 
        * check the return value or an exception 
        * check a state change(use a public API)
        * check a method call(use a mock or spy)
         
    + a good unit test should only have on reason to fail 
    + mock can contain issues and fail the test which is different from stub and fake object 
        
- example using a spy, test spy
MyServiceTest -> Spy kind of test double which can listen the class you test and collaborate 
            |
MyService --o--- SSORegistery

use spy like mock, mock will blow up straight away but spy will record the fail value and use it during the assert statement 
    

def test_valid_token_with_spy(self):
    token = SSOToken
    registry = SpySingleSignOnRegistry(accept_all_token=True)
    my_service = MyService(registry)
    
    response = my_service.handle_request("do stuff", token=token)
    self.assertIn(token, registry.checked_tokens)
    
class SpySingleSignOnRegistry:
    def __init__(self, accept_all_tokens = True):
        self.accept_all_tokens = accept_all_tokens
        self.checked_tokens = []
    
    def is_valid(self, token):
        self.check_tokens_append(token)
        return self.accept_all_tokens
        
- using unittest.mock to create mock and spy 
- example using a dummy object, basic kind of test double, use a dummy when you have require object that is not need during testing
dummy object is the one you need to provide a argument but it is not really needed 
def foo(para):
    pass
    '''
    >>>foo(None)
    '''
the None is the dummy object 

- choosing to use a particular kind of test double 
    + a stub returns a hard coded answer to any query 
    + a fake is real implementation, yet unsuitable for production 
    + a mock is as a stub, and additionally verifies interactions 
    + a test spy lets you query afterwards to find out what happened 
    + a dummy is for when the interface requires an argument 
    
    + what is unit testing for 
        * understand what to build 
        * design the units 
        * document the unit 
        * regression protection 
        
    + isolation each units and test 
    
- monkeypatching, is when you dynamic change a attribute during run time 
    + changing code at runtime! some people called it meta programming 
    + when you test the code which is design good for testing 
    
    + like the code is not use dependency injection 
    class Alarm(object):
        def __init__(self):
            self._low_pressure_threshold = 17 
            self._high_pressure_threshold = 21 
            self._sensor = Sensor()
            self._is_alarm_on = False
        def check(self):
            psi_pressure_value = self._sensor.sample_pressure()
            if psi_pressure_value<self._low_pressure_threshold or slef._high_pressure_threshold<psi_pressure_value:
                self._is_alarm_on = True 
                
        @property 
        def is_alarm_on(self):
            return self._is_alarm_on 
            
    import unittest 
    from unittest.mock import *
    from alarm import Alarm
    
    class AlarmTest(unittest.TestCase):
        with patch('alrm.Sensor') as test_sensor_class:
            test_sensor_instance = Mock()
            test_sensor_instance.sample_pressure.return_value = 22 
            test_sensor_class.return_value = test_sensor_instance
            alrm = Alarm()
            alarm.check()
            self.asserTrue(alarm.is_alarm_on)
        
        
# Test Coverage and Parametrized Tests 
- tennis, using a custom assert to reduce duplication 
    + love-All
    + fifteen-love 
    + fifteen-thirty 
    + fifteen-forty 
    
use a custom function wrapper some assert condition to do the test 

- defining parametrized tests with unittest 
    
    + use meta programming to dynamic generate test 
    
    use test data to generate test method 
import unittest 
from tennis import tennis_score 

test_case_data = \
{
    "even_scores": [("love-All", 0, 0),
                    ("Fifteen-All", 1, 1)],
    "early_games_with_uneven_scores": [("Love-Fifteen", 0, 1),
                                        ("Fifteen-Love", 1, 0)]
}

def tennis_test_template(*args):
    def foo(self):
        self.assert_tennis_score(*args)
    return foo
    
for behaviour, test_cases in test_case_data.items():
    for tennis_test_case_data in test_cases:
        expected_output, player1_score, player2_score = tennis_test_case_data 
        test_name = "test_{0}_{1}_{2}", format(behavior, player1_score, player2_score)
        tennis_test_case = tennis_test_template(*tennis_test_case_data)
        setattr(TennisTest, test_name, tennis_test_case)
    
    
class TennisTest(unittest.TestCase):
    
    def assert_tennis_score(self, expected_score, player1_points, player2_points):
        self.assertEqual(expected_score, tennis_score(player1_points, player2_points)
    
    
then the test cases are automatic generate by meta programming 
    
this is data driven testing programming model 

- defining parametrized tests with pytest 

examples = (("expected_score", "player1_points", "player2_points", "optional test comments"),
            [("Love-All",     0,                 0),
             ("Fifteen-All",  1,                 1),
             ("Thirty-All",   2,                 2)])
                           
@pytest.mark.parametrize(*exmples)
def test_early_game_scores_equal(expected_score, player1_points, player2_points):
    assert expected_score == tennis_score(player1_points, player2_points)

def test_early_game_scores_equal_ordinary():
    assert "Love-All" == tennis_score(0, 0)
    assert "Fifteen-All" == tennis_score(1, 1)
    assert "Thirty-All" == tennis_score(2, 2)
    
- measuring coverage, coverage for tennis 
$ pip install coverage 
$ pip install pytest-cov 

$ python -m pytest --cov-report term-missing --cov tennis 
report to terminal

$ python -m pytest --cov-report html --cov tennis 
report as html which will also display the test coverage 
    
could mark a function for the aim class to not calculate in the test coverage 
    
def any_method_don't_want_to_be_test(): # pragma: no cover 
    pass
    
- measuring coverage of unittest tests 
$ python -m coverage run -m unittest 
$ python -m coverage report 
$ python -m coverage html  
    
could click the module name to check the test coverage detail 
    
- using coverage data to add tests to legacy code 

    + create a initial test before refactor a legacy code 
    + generate the coverage report to check which codes are tested 
    + use branch coverage will given more detail about the coverage 
        * with python 
        * create configurable 
        .coveragerc file with 
        [run]
        branch = True

    branch refers to the fact that in an if-statement or other conditional there are two possible out come. If any condition output is not coverage the conditional statement will be marked with yellow 

- good and bad uses for coverage metrics 
    + finding missing test cases 
    + get legacy code under test 
    + continuous integration - constant measurement
    
    in a real project some parts of the parts may have less coverage, some of them may need manually test, some of may required 100% percent test, normally 80% is set to a threshold 

    test quality != coverage 










    
    
    
    
    
    
    
    






        