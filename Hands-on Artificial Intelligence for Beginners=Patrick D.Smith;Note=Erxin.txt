Hands-on Artificial Intelligence for Beginners=Patrick D.Smith;Note=Erxin

# History 
- perceptron algorithm
The input layer: The initial layer for reading in data
Weight and biases vectors: Weights help learn appropriate values during training for the connections between neurons, while biases help shift the activation function to fit the desired output
A summation function: A simple summation of the input
An activation function: A simple mapping of the summed weighted input to the output

- By the late 1960s, funding in the US and the UK began to dry up. In 1969, a book named Perceptrons by MIT's Marvin Minsky and Seymour Papert (https://archive.org/details/Perceptrons)

-  University of Cambridge who persisted in his research, would make a name for himself by coining the term deep learning.

-  long short-term memory unit (LSTM), which is still used today for sequence modeling.
- Computing and storage limitations prevented these networks from scaling, and other methods such as support vector machines (SVMs)

- Interestingly enough, Hinton's team's work on computer vision also introduced the idea of utilizing Graphics Processing Units (GPUs) to train deep networks.


# Technical requirements 
- install sklearn 
$ pip install sklearn 

- ANN, artificial neural network 
- element wise multiplication 

import numpy as np## Element-wise multiplication without utilizing linear algebra techniquesx = [1,2,3]y = [4,5,6]product = []for i in range(len(x)):    product.append(x[i]*y[i])

- scalars, real numbers 
- vectors 
my_vector = np.array([5,6])

- we discuss principal component analysis (PCA) in the next few pages
- matrices 
matrix = np.array([[5,6], [6,9]])

- tensors,  A tensor is a generalized matrix, and they have different sizes, or ranks, which measure their dimensions.
tensor = [[[1,2,3,4]],[[2,5,6,3]],[[7,6,3,4]]] 

- matrix math 
    + sclar operations 
vector = np.array([[1,2], [1,2]])new_vector = vector + 2

    + element wise operations, multiplication that we may perform with vectors
    dot product, Our x and w are both matrices, and we need to output a scalar , which represents either cat or dog. We can only do this by taking the dot product of w and .
    
    hadmard product, Hadamard product, on the other hand, outputs a vector
    [a  o [c = [a . c]
    b]    d]   [b . d]
    
    The Hadamard product is element-wise, meaning that the individual numbers in the new matrix are the scalar multiples of the numbers from the previous matrices
    vector_one = np.array([1,2,3])
    vector_two = np.array([2,3,4])
    vector_one * vector_two
    ## You should see:
    array([ 2,  6, 12])
    
    inner product 
    a*c + b*d 

- basic statistics and probability theory 
The sample space, which tells us the possible outcomes or a situation 
A defined set of events; such as two fraudulent credit card transactions
The measure of probability of each of these events

probability space 
Independence is a state where a random variable does not change based on the value of another random variable



















