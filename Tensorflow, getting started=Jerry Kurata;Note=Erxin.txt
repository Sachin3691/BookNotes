Tensorflow, getting started=Jerry Kurata;Note=Erxin

# Course overview 
- example 
virtual asssitants 

automatic recognition 

self-driving vehicles 

- tesnsor flow 
interface for expressing machine learning algrotihms 

implementation for executing such algorithms 

a framework for creating ensemeble algorithms for todays's most challenging problems 

- tensorflow as an interface 
lower level library 

flexible structure

predefined implementation available 

the source code could be download 

- tensorflow as an implementation for execution 
scales up and down 

easier deployment to production, easy move from laptop to claster 

- tensor flow environment 
languages interfaces, python, cpp

execution environment, execution master use the process we add or remote server 

- why is called tensorflow? 
computation graph of (a*b)+c 

    + tensor is n-dimention structure
    n=0, single value 
    n=1, list of values 
    n=2, matrix of values 

    the tensor could do normal math operation 
    
    
    
    tensors flowing between operations => "tensorflow"
    
- skills and course structure 
not required                | required 
experience in tensorflow    | software experimence 
python 3.5                  | experimence with data in tables 
IDE, visual studio code     | 
advanced statics or math    |

- course modules 

use a computation graph on local or remote 

introducing tensorflow 

creating neural networks 

debugging and monitoring 

transfer learning, use the previous trained model to find data

expanding with add-ons, Keras and TFLearn, less on managing data with tensor flow 

summary 


# Creating neural networks in tensorflow 
- install 

creating models in tensorflow 

discussing tensorflow architecture 
    
    +steps 
    OS 
    GPU 
    Environment 
    Direct, Virtual or Docker Container 
    Python version 
        Linx 2.7, 3.3 
        Window 3.5 or later 
    Demo 
    
    + build a simple neural network 
    
    + creating deep neural networks 
    
    + tensorflow make the neural network simple 

- introduce neural networks 
    + combine the features together we could define a object 
    
    + neuron architecture 
    
    inputs          weights        
    I_1------------>w_1---+
    I_2------------>w_2---+
    I_3------------>w_3---+--sum --> activate __phi__ --> output 
                          |
                    bias--+ 
                    
    bias specify the contribution for a specific neuron 
        sum(I*W) + Bias         Act(sum(I * W) + Bias)
        
    + neural network learning 
    compute loss between the output and the real value 
    optimize the bias and weight to reduce the loss 
    
- simple MNIST 
    + handwritten digit recognition, MNIST dataset 
    yann.lecun.com/exdb/mnist 
    
    classic testing set 
    
    70000 data points 
        * 55000 training 
        * 10000 test 
        * 5000 validation 
        
    each data point contains 
    28x28 grayscale image 
    label the digit 0-9 
    
    + training a neural network with tensorflow 
    concept 
    prepared data 
    inference 
    loss measurement 
    optimize to minimize loss 
    
    + implementation 
    MINST 
    sum(x*weight) + bias -> activation 
    cross entropy 
    gradient descent optimizer 
    
    + sample 
import tensorflow as tf 
from tensorflow.examples.tutorials.mnist import input_data 

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

//#placeholder for the 28x28 image data 
x = tf.placeholder(tf.float32, shape=[None, 784])

//# y_ is called "y bar" and is a 10 element vector containing the predicted probability of each digit(0-9) class such as [0.14, 0.8, 0, ...]
y_ = tf.placeholder(tf.float32, [None, 10])

//#define weights and balances 
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

//#define our model 
y = tf.nn.softmax(tf.matmul(x, W) + b)
    
    
//# loss is cross entropy 
cross_entropy = tf.reduce_mean(
tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))

//#each training step in gradient decent we want to minimize cross entropy 
train_step = tf.train.GradientDescendentOptimizer(0.5).minimize(cross_entropy)

//#initizile the global variables 
init = tf.global_variable_initializer()

//# create an interactive session that can span mulitple code blocks 
sess = tf.Session()

//# perform initialization of all global variables 
sess.run(init)

//# Perform 1000 training steps 
for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100) # get 100 random data points from the data. batch_xs = image 
    
    //# optimize the with this data 
    sess.run(train_step, feed_dict = {x:batch_xs, y_:batch_ys}) 
    
//# evalute how well the mode 
correct_prediction = tf.equal(tf.argmx(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
test_accuracy = sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.lables})
print("Test accuracy: {0}%".format(test_accuracy * 100.0))

sess.close()

    + result is around 90% we could use deep MNIST to improve the accuracy 
- deep MNIST 
    + consider data 
    we have images 
    
    insert convolution network layers 
    + inspects subsets of images
    
    we use
    
- neural network symbology 












