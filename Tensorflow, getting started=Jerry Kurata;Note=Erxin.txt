Tensorflow, getting started=Jerry Kurata;Note=Erxin

# Course overview 
- example 
virtual asssitants 

automatic recognition 

self-driving vehicles 

- tesnsor flow 
interface for expressing machine learning algrotihms 

implementation for executing such algorithms 

a framework for creating ensemeble algorithms for todays's most challenging problems 

- tensorflow as an interface 
lower level library 

flexible structure

predefined implementation available 

the source code could be download 

- tensorflow as an implementation for execution 
scales up and down 

easier deployment to production, easy move from laptop to claster 

- tensor flow environment 
languages interfaces, python, cpp

execution environment, execution master use the process we add or remote server 

- why is called tensorflow? 
computation graph of (a*b)+c 

    + tensor is n-dimention structure
    n=0, single value 
    n=1, list of values 
    n=2, matrix of values 

    the tensor could do normal math operation 
    
    
    
    tensors flowing between operations => "tensorflow"
    
- skills and course structure 
not required                | required 
experience in tensorflow    | software experimence 
python 3.5                  | experimence with data in tables 
IDE, visual studio code     | 
advanced statics or math    |

- course modules 

use a computation graph on local or remote 

introducing tensorflow 

creating neural networks 

debugging and monitoring 

transfer learning, use the previous trained model to find data

expanding with add-ons, Keras and TFLearn, less on managing data with tensor flow 

summary 


# Creating neural networks in tensorflow 
- install 

creating models in tensorflow 

discussing tensorflow architecture 
    
    +steps 
    OS 
    GPU 
    Environment 
    Direct, Virtual or Docker Container 
    Python version 
        Linx 2.7, 3.3 
        Window 3.5 or later 
    Demo 
    
    + build a simple neural network 
    
    + creating deep neural networks 
    
    + tensorflow make the neural network simple 

- introduce neural networks 
    + combine the features together we could define a object 
    
    + neuron architecture 
    
    inputs          weights        
    I_1------------>w_1---+
    I_2------------>w_2---+
    I_3------------>w_3---+--sum --> activate __phi__ --> output 
                          |
                    bias--+ 
                    
    bias specify the contribution for a specific neuron 
        sum(I*W) + Bias         Act(sum(I * W) + Bias)
        
    + neural network learning 
    compute loss between the output and the real value 
    optimize the bias and weight to reduce the loss 
    
- simple MNIST 
    + handwritten digit recognition, MNIST dataset 
    yann.lecun.com/exdb/mnist 
    
    classic testing set 
    
    70000 data points 
        * 55000 training 
        * 10000 test 
        * 5000 validation 
        
    each data point contains 
    28x28 grayscale image 
    label the digit 0-9 
    
    + training a neural network with tensorflow 
    concept 
    prepared data 
    inference 
    loss measurement 
    optimize to minimize loss 
    
    + implementation 
    MINST 
    sum(x*weight) + bias -> activation 
    cross entropy 
    gradient descent optimizer 
    
    + sample 
import tensorflow as tf 
from tensorflow.examples.tutorials.mnist import input_data 

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

//#placeholder for the 28x28 image data 
x = tf.placeholder(tf.float32, shape=[None, 784])

//# y_ is called "y bar" and is a 10 element vector containing the predicted probability of each digit(0-9) class such as [0.14, 0.8, 0, ...]
y_ = tf.placeholder(tf.float32, [None, 10])

//#define weights and balances 
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

//#define our model 
y = tf.nn.softmax(tf.matmul(x, W) + b)
    
    
//# loss is cross entropy 
cross_entropy = tf.reduce_mean(
tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))

//#each training step in gradient decent we want to minimize cross entropy 
train_step = tf.train.GradientDescendentOptimizer(0.5).minimize(cross_entropy)

//#initizile the global variables 
init = tf.global_variable_initializer()

//# create an interactive session that can span mulitple code blocks 
sess = tf.Session()

//# perform initialization of all global variables 
sess.run(init)

//# Perform 1000 training steps 
for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100) # get 100 random data points from the data. batch_xs = image 
    
    //# optimize the with this data 
    sess.run(train_step, feed_dict = {x:batch_xs, y_:batch_ys}) 
    
//# evalute how well the mode 
correct_prediction = tf.equal(tf.argmx(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
test_accuracy = sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.lables})
print("Test accuracy: {0}%".format(test_accuracy * 100.0))

sess.close()

    + result is around 90% we could use deep MNIST to improve the accuracy 
- deep MNIST 
    + consider data 
    we have images 
    
    insert convolution network layers 
    + inspects subsets of images
    
    we use
    
# Debugging and Monitoring 
- unique features 
    + run in session context 
    python debugger limited usefulness 
    
    + session locality varies 
    local CPU 
    local GPU remote system
    remote cluster 
    
    + machine learning issues 
    complex models 
    concern about data and trends 
    long run times and generate huge data 
    
    + tensorflow features and tools 
    Name of graph 
    Name Scope, tensorflow have summary class and help us log tensor changes 
    Summary methods 
    TensorBoard 
- using names and name scope 
we can add name property for each tensors 

    + when we want to add names for different layers we could use 
    
with tf.name_scope('name'):
    xxx
    xxx 
    
- introducing tesnsorboard 
    + visualizing learning 
    + visualize computation graph 
    + monitor performance metrics 
    + adding support for tensorboard 
    define log file location 
    define names and name scopes 
    good name and anme scope definition 
    add summary methods 
    train 
    run tesnorboard, it's a web server to show our collected data 
    
- using tensorboard, define folder to hold the log files in the tensorflow code 

logPath = './tb'
    + add summary 
    
def variable_summaries(var):
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean', mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.squire(var - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))
        tf.summary.histogram('histogram', var)
        
call the function to collect data 

with tf.name_scope('weights'):
    W_con2 = weight_variable([5,5,1,32], name='weight')
    variable_summaries = (W_con1)
        
    + run tensorboard command with the log path 
$ Remove-Item log_file_path
$ tensorboard --log log_file_path 

    
# transfer learning with tensorflow 
- Inception, image recognition engine provided by google 

provied in Tensorflow 
Super human image classification 
    Human - 5.1% error 
    inception v3 - 3.46 % error 
    
    Traning takes a long time 
    2 week 
    8 Nvidia k40 processors 
    
    trained on ImageNet dataset 
- transfer learning basics 
    + get our data 
    + load inception 
    + retrain next to last (bottleneck) layer 
    + replace last layer to output of our classes 
- implementing transfer learning 
https://github.com/JerryKurata/GettingStartedWithTensorFlow-TransferLearning 

retrain.py - retrains inception 
flower_photos folder, images used by retrain.py 
try-retrain.py test retrained model 
test-images folers - images used by try-retrain.py 

clone the repo and check the contents 

- retraining inception 
- using our retrained model 
- summary 

# extending tensorflow with add-ons, high level abstraction and hide the data tranformation and layers 
- Keras, on top of Tensorflow or Theano 

Focused on Neural Networks 

Soon part of Tensorflow

Expanding support to other frameworks 

    + using keras 
    define model -> create your own relationships between layers -> add layers -> compile tensorflow objects are created -> train -> evaluate 
- Using Keras, separate installation may not be needed in future 
instructions at keras.io 
pip installation available 

$ pip install keras 
$ python -c "from keras import backend; print(backend._BACKEND)"

from keras.models import Sequential
from keras.layers.core import Dense, Activation 

model = Sequential()
mode.add(Dense(1, input_shape=(1,), init='uniform', activation='linear'))
model.compile(loss='mean_squared_error', optimizer='sgd')
model.fit(train_house_size_norm, train_price_norm, nb_epoch=300)

score = model.evaluate(test_host_size_norm, test_house_price_norm)
print('lost on test:{0}'.format(score))

- DeepMNIST in Keras 
from keras.models import Sequential
from keras.layers import Dense 
from keras.layers import Convolution2D, MaxPooling2D 
from keras import backend as K 

model = Sequential() 
mode.add(Convolution2D(num_filters, conv_kernel_size[0], conv_kernel_size[1], border_mode='valid', input_shape_imag_shape))
//push through RELU activation 
model.add(Activation('relu'))
//take results and run through max_pool 
model.add(MaxPooling2D(pool_size=max_pool_size))

//2nd convolution layer 
model.add(Convolution2D(num_filters, conv_kernel_size[0], conv_kernel_size[1])
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=max_pool_size))

//fully connected layer
model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))

#dropout 
model.add(Dropout(drop_prob))


//readout layer 
model.add(Dense(num_classes))
model.add(Activation('softmax')) 

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
              
model.fit(train_images, mnist.train.labels, batch_size=batch_size, nb_epoch_num_epoch, verbose=1, validation_data=(test_images, mnist.test.labels))


- TFLearn 
only supports tensorflow 
part of TF distribution 
uses tensors 
callable from tensorflow 
- Using TFLearn 
define layers -> load model -> train -> evaluate 

    + two types of neural networks
deep neural network are layers connected layers 
general neural network which is used for prediction 
- DeepMNIST in TFLearn
- Summary 
keras target multiple audience, modular approach. data in numpy or python types 

tflearn, only support tensorflow, add classeson top of tensorflow, uses tensors for data directly tranfered, tflearn functions can be called from tensorflow

keras and tflearn
    + reduce coding 
    + reduced errors 

other libraries from tensorflow repo 

- summary 
http://wwww.deeplearningbook.org 
http://keras.io 
http://tflearn.org 










