Node in Practice=Alex Young;Note=Erxin

# Node fundamentals 
- getting started 
    + microsoft developed webmatrix which directly support node 
    
    + node web framework, http://expressjs.com 
    
    + when to use node 
    advertising distribution 
    game server 
    content management system, blog 
    
    + main festures 
    app.js 
    core module 
    c++ buildings 
    libuv, c-ares, http  | v8 
    os 
    
    every node developers runs into EventEmitter, it's the actually the basis for most of node's core modules 
    
    + stream, the basis for scalable i/o, using node's stream api allows you to create an object that receives events about the connection 
    
    + get information about files with fs.stat and synchronous equivalent is fs.statSync 
    
    use fs.createReadSream to return a ReadableStream 
    
- building a node application 
- creating a new node project file.js 
    + create a new directory, cd into it and then run npm init
    $ npm init 
    
    + making a stream class 
    var writable = require('stream').writable;
    var util = require('util');
    
    module.exports = CountSteam;
    
    util.inherits(CountStream, writable);
    
    function CountSteam(matchText, options){
        writable.call(this, options);
        this.count = 0; 
        this.matcher = new RegExp(matchText, 'ig'); //create a regexp object 
    }
    
    CountStream.prototype._write = function (chunk, encoding, cb){
        var matches = chunk.toString().match(this.matcher);
        if(matches){
            this.count += matches.length;
        }
        cb();
    }
    
    CountSteam.prototype.end = function(){
        this.emit('total', this.count);
    };
    
    // using the stream 
    var CountStream = require('./countstream');
    var countStream = new CountStream('book');
    var http = require('http');
    http.get('http://www.manning.com', function(res) {
    res.pipe(countStream);
    });
    countStream.on('total', function(count) {
    console.log('Total matches:', count);
    });
    
    + writing a test 
    var assert = require('assert');
    
    update the test property of package.josn to 
    "scripts":{
        "test": "node test.js"
    },

- globals node's environment 
    + global process object 
    + node's Buffer class 
    + globals, in module scope 
    
    + modules, can be used to organize larger programs and distribute, load module use require 
    
    $ npm search express 
    
    $ npm install express 
    
    var express = requre('express');
    
    global module 
    /sur/local/lib/node_modules on *nix 
    
    npmsearch command which created by Gorgi Kosev will order results using its own relevance rankings 
    
    + creating and managing modules 
    use the exports object, node have a built-in module system base on CommonJS Modules/1.1 
    http://wiki.commonjs.org/wiki/Modules/1.1/
    
    function MyClass(){
    
    }
    
    MyClass.prototype = {
        method: function(){
            return 'Hello';
        }
    };
    
    var myClass = new MyClass();
    module.exports = myClass;
    exports.method = function(){
    };
    
    node will attempt to find a module in $NODE_PATH then ./node_modules $HOME/.node_modules 
    
    + unloading modules from require.cache object 
    delete require.cache[require.resolve('loaded-module-path')];
    
    + loading a group of related modules 
    
    ndoe's module system supports by allowing directories to act as modules. the easiest way to do this is to create a file cached index.js that has a require statement to load each file. 
    module.exports = {
        one: require('./one');
        two: require('./two');
    };
    
    if no package.json is present, it'll then look for index.js 
    
    working with paths, solution use __dirname or __filename to determine the location of the file 
    
    + standard i/o and the console object 
    
    + reading and writing to standard i/o 
    process.stdout 
    process.stdin 
    process.stdin.resume(); //tell the stream we're ready to start reading 
    process.stdin.setEncoding('utf8');
    
    process.stdin.on('data', function(id){
        process.stdout.write(d);
    });
    
    + logging message 
    console.log, console.info, console.error and console.warn 
    
    format placeholders 
    %s, string 
    %d, number
    %j, json 
    
    + benchmarking a program 
    console.time(), console.timeEnd() 
    
    var stream = require('fs').createReadStream(file);
    
    stream.on('end', function(){
        console.timeEnd('read');
    });
    
    third party benchmark module, https://npmjs.org/package/benchmark, microtime 
    
    + operating system and command-line integration 
    process.arch and process.platform 
    'x64', 'ia32', ... 
    
    the process's current memory usuage 
    rss, resident set size 
    heapTotal 
    heapUsed 
    
    + passing commandline arguments 
    process.argv 
    
    node argument.js -r arguments.js 
    
    + exit program 
    process.exit()
    
    + script use mongoose database library, http://mongoosejs.com need to call mongoose.connection.close() before process will be able to exit 
    
    + you need to response to signals sent by other process, use the signal events that send to the process object 
    
    add a POSIX signal name should work for unix system, type man sigaction to see the names of all of the signals 
    process.stdin.resume(); //read from stdin so the program will run until ctrl-c is pressed or it's kill 
    process.on('SIGHUP', function() {
        console.log('reloading configuration');
    });
    
    process.kill(pid, [signal]);
    
    + delaying execution with timers 
    
    setTimeout 
    
    setInterval 
    
    + managing asynchronous api 
    var EventEmitter = require('events').EventEmitter;
    
    process.nextTick to wrap the synchronous operations 
    process.nextTick(function(){
        events.emit('success');
    });
    
    setImmediate and clearImmediate global functions accept a callback and optional arguments and will run after any upcoming i/o events but before setTimeout and setInterval
    
    callbacks added this way are pushed onto a queue and one callback will be executed per run loop this is different from process.nextTick, which causes process.maxTickDepth callbacks to run 
    
    process.nextTick is usually run at the end to current event loop 
    
    
- buffers 
    + buffers are raw allocations of the heap, exposed to javascript in an array-like manner 
    + changing data encoding 
    Buffer.isBuffer(bf);
    
    var fs = require('fs');
    fs.readFile('./file.txt', function(err, buf){
        console.log(buf.toString());
    });
    
    + changing string encoding using buffers 
    
    var buf = new Buffer('string', 'encoding-type');
    
    + converting binary files to json, DBase 5.0 header specification *.dbf file 
    
    var date = new Date();
    date.setUTCFullYear(1900 + buf[1]);
    date.setUTCMonth(buf[2]);
    date.setUTCDate(buf[3]);
    header.lastUpdated = date.toUTCString();
    
    buf.readUInt8()

- events 
    + basic usage, the base class must be inherit from EventEmitter
    var util = require('util');
    var events = require('events');
    function MusicPlayer() {
    events.EventEmitter.call(this);
    }
    util.inherits(MusicPlayer, events.EventEmitter);
    
    var musicPlayer = new MusicPlayer();
    musicPlayer.on('play', function(track) {
    this.playing = true;
    AudioDevice.play(track);
    });
    musicPlayer.on('stop', function() {
    this.playing = false;
    AudioDevice.stop();
    });
    musicPlayer.emit('play', 'The Roots - The Fire');
    setTimeout(function() {
    musicPlayer.emit('stop');
    }, 1000);
    
    the util.inerits is the easiest and most common way to create customized event-based classes 
    
    //removing listeners 
    function play(track){
        this.playing = true;
    }
    musicPlayer.removeListener('play', play);
    
    musticPlayer.once('play', {
        this.audioFirstStarted = new Date();
    });
    
    + EventEmitter methods, on, emit and removeListener
    
    + mixing in EventEmitter
    var EventEmitter = reuquire('events').EventEmitter;
    
    + error handling 
    source.on('error', function(err){
    });
    
    source.emit('error', options);
    
    an error event are treated as a special case in Node, the default action is print a stack and exit the program 
    
    + managing errors with domains, node's domain API provides a way of wrapping existing non-blocking APIs and exceptions with error handlers. this helps centralize error handling and is particularly useful
    
    var util = require('util');
    var domain = require('domain');
    var events = require('events');
    var audioDomain = domain.create();
    
    function AudioDevice() {
        events.EventEmitter.call(this);
        this.on('play', this.play.bind(this));
    }
    util.inherits(AudioDevice, events.EventEmitter);
    
    AudioDevice.prototype.play = function() {
        this.emit('error', 'not implemented yet');
    };
    
    function MusicPlayer() {
        events.EventEmitter.call(this);
        this.audioDevice = new AudioDevice();
        this.on('play', this.play.bind(this));
        this.emit('error', 'No audio tracks are available');
    }
    util.inherits(MusicPlayer, events.EventEmitter);
    
    MusicPlayer.prototype.play = function() {
        this.audioDevice.emit('play');
        console.log('Now playing');
    };
    
    audioDomain.on('error', function(err) {
        console.log('audioDomain error:', err);
    });
    
    audioDomain.run(function() {
        var musicPlayer = new MusicPlayer();
        musicPlayer.play();
    });
    
    + advanced patterns, reflection, for dynamic respond to changes to an instance of an EventEmitter
    
    track when listeners are added, emit a new-listener event 
    var util = require('util');
    var events = require('events');
    function EventTracker() {
    events.EventEmitter.call(this);
    }
    util.inherits(EventTracker, events.EventEmitter);
    var eventTracker = new EventTracker();
    eventTracker.on('newListener', function(name, listener) {
    console.log('Event name added:', name);
    });
    eventTracker.on('a listener', function() {
    // This will cause 'newListener' to fire
    });
    
    + use rabbitmq with node
    + use zeromq with node 
    
- streams 
    + introduce to streams, in node streams are an abstract interface adhered to by several different objects. it means a way of doing things 
    
    streams can be read, write and implemented with EventEmitter, it provide creating data flows between objects. can be composed LEGO-like modularity 
    
    + types of streams, stream handbook https://github.com/substack/stream-handbook/
    built-in, such as fs.createReadSream
    http 
    parsers, such as xml or json 
    browser, node's event-based streams have been extended to work in browsers 
    audio 
    RPC
    test 
    control, meta and state 
    
    + when to use streams, stream is asynchronous by design, rather than reading entire file into memory a buffer's worth will be read, the desired operations will be performed and then result will be written to the output stream 
    
    node is like unix pipes 
    
    + streams in thrid party modules, streams2 
    stream.Readable, _read(size)
    stream.Writable, _write(chunk, encoding, callback)
    stream.Duplex, _read(size), _write(chunk, encoding, callback)  a readable and writable stream like a network connection 
    stream.Transform, _flush(size), _transform(chunk, encoding, callback), a duplex stream that changes data in some way 
    
    the Mongoose MongoDB module, http://mongoosejs.com 
     
    the pipe and unpipe events are emitted when passing a stream to the stream.Readable.prototype.pipe method 
    
    + built-in streams 
    
        * send a file from a web server to a client in an effcient manner 
        fs.createReadStream to open a file and sream it to the client 
        
        var http = require('http');
        var fs = require('fs');
        
        http.createServer(function(req, res){
            fs.readFile(__dirname + '/index.html', function(err, data){
                if (err){
                    res.statusCode = 500;
                    res.end(String(err));
                }
                else{
                    res.end(data);
                }
            });
        }).listen(8000);
     
    replace the above code with fs.createReadStream will significate improve the performance 

    + demonstrates a streaming static web server 
    var http = require('http');
    var fs = require('fs');
    http.createServer(function(req, res) {
        fs.createReadStream(__dirname + '/index.html').pipe(res);
    }).listen(8000);
     
    use gzip to response 
     
     var http = require('http');
     var fs = require('fs');
     var zlib = require('zlib');
     http.createServer(function (req, res) {
     	res.writeHead(200, {
     		'content-encoding' : 'gzip'
     	});
     	fs.createReadStream(__dirname + '/index.html')
     	.pipe(zlib.createGzip())
     	.pipe(res);
     }).listen(8000);
     
    general pattern is readable.pipe(writable);
     
    + handle error, add error listener 
    var fs = require('fs');
    var stream = fs.createReadStream('not-found');
    stream.on('error', function (err) {
    	console.trace();
    	console.error('Stack:', err.stack);
    	console.error('The error raised was:', err);
    });
     
    console.trace() will show a trace up to the ReadStream implementation in Node’s events.js core module.
     
    + third party modules and streams, the express web framework, http://expressjs.com/ it provides a relatively lightweight wrapper around node's core http module 
    
    it includes the Request and Response objects 
    
    Express route a callback that runs for a given http method and url, uses res.send to respond with some data 
    
    var express = require('express');
    var app = express();
    app.get('/', function (req, res) {
    	res.send('hello world');
    });
    app.listen(3000);
     
    req, is a Request object 
    res, is a Response object 
    
    //Express 3 and streams content from a custom readable stream by using pipes 
    var stream = require('stream');
    var util = require('util');
    var express = require('express');
    var app = express();
    util.inherits(StatStream, stream.Readable);
    function StatStream(limit) {
    	stream.Readable.call(this);
    	this.limit = limit;
    }
    StatStream.prototype._read = function (size) {
    	if (this.limit === 0) {
    		// Done
    		this.push();
    	} else {
    		this.push(util.inspect(process.memoryUsage()));
    		this.push('n');
    		this.limit--;
    	}
    };
    app.get('/', function (req, res) {
    	var statStream = new StatStream(10);
    	statStream.pipe(res);
    });
    app.listen(3000);
     
    + use stream with mongoosejs 
    User
        .where('role')
        .equals('admin')
        .stream()
        .pipe(writeStream);
     
    + use stream with mysql 
    var query = connection.query('SELECT * FROM posts');
    query
    .on('result', function (row) {
    	connection.pause();
    	processRow(row, function () {
    		connection.resume();
    	});
    });
     
    + using the strem base classes, provide templates for solving the kinds of problems that streams are best at stream.Transform is great for parsing data and stream.Readable is perfect for wrapping lower-level apis 
    
    + correctly inheriting from the stream base classes, not sure which base classes to use and how to use it 
    
    then use Object.prototype.call and util.inherites 
    
    the base class are abstract class need to implement method
    
    five basic class Readable, Writable, Duplex, Transform and PassThrough 
    
    Transform is used when change data in some way by parsing it 
    PassThrough is used when extract data from streams without changing it from testing to analysis 
    
    inherit from the base class
    MyStream.prototype = new Stream.Readable() 
    
    var Readable = require('stream').Readable;
    function MyStream(options) {
    	Readable.call(this, options);
    }
    MyStream.prototype = Object.create(Readable.prototype, {
        constructor : {
            value : MyStream
        }
    });
     
    Object.create will correctly setup the prototype chain 
    
    in the case of readable the options are 
    highWaterMark, the number of bytes to store in the internal buffer before pausing reading from the underlying data source 
    encoding, causes the buffer to be automatically decoded, possible values include utf8 and ascii 
    objectMode, allows the stream to behave as a stream of objects rather than bytes 
    
    + implement a readable stream, provide higher-level interface that would otherwise be possible with underlying data. inherit from stream.Readable and creating a _read(size) method 
    
    var stream = require('stream');
    var util = require('util');
    var fs = require('fs');
    
    function JSONLineReader(source) {
    	stream.Readable.call(this);
    	this._source = source;
    	this._foundLineEnd = false;
    	this._buffer = '';
    	source.on('readable', function () {
    		this.read();
    	}).bind(this));
    }
    
    util.inherits(JSONLineReader, stream.Readable);
    
    JSONLineReader.prototype._read = function (size) {
    	var chunk;
    	var line;
    	var lineIndex;
    	var result;
    	if (this._buffer.length === 0) {
    		chunk = this._source.read();
    		this._buffer += chunk;
    	}
        
    	lineIndex = this._buffer.indexOf('\n');
        
    	if (lineIndex !== -1) {
    		line = this._buffer.slice(0, lineIndex);
    		if (line) {
    			result = JSON.parse(line);
    			this._buffer = this._buffer.slice(lineIndex + 1);
    			this.emit('object', result);
    			this.push(util.inspect(result));
    		} else {
    			this._buffer = this._buffer.slice(1);
    		}
    	}
    };
    
    var input = fs.createReadStream(__dirname + '/json-lines.txt', {
    		encoding : 'utf8'
    	});
        
    var jsonLineReader = new JSONLineReader(input);
    
    jsonLineReader.on('object', function (obj) {
    	console.log('pos:', obj.position, '- letter:', obj.letter);
    });
     
    + a stream configured to use objectMode 
    var stream = require('stream');
    var util = require('util');
    util.inherits(MemoryStream, stream.Readable);
    
    function MemoryStream(options) {
    	options = options || {};
    	options.objectMode = true;
    	stream.Readable.call(this, options);
    }
     
    MemoryStream.prototype._read = function (size) {
    	this.push(process.memoryUsage());
    };
    var memoryStream = new MemoryStream();
    memoryStream.on('readable', function () {
    	var output = memoryStream.read();
    	console.log('Type: %s, value: %j', typeof output, output);
    });
     
    + implement a writable stream by implement _write method  
    MyWritable.prototype._write = function (chunk, encoding, callback) {
    	this.customWriteOperation(chunk, function (err) {
    		callback(err); //
    	});
    }
     
        * chunks and encodings encoding argument is only relevent when strings are being ued instead of buffers, strings can be used by setting decodingStrings to false in the option 
     
    + duplex will need implement _read and _write 
    var stream = require('stream');
    HungryStream.prototype = Object.create(stream.Duplex.prototype, {
    		constructor : {
    			value : HungryStream
    		}
    	});
    function HungryStream(options) {
    	stream.Duplex.call(this, options);
    	this.waiting = false;
    }
    HungryStream.prototype._write = function (chunk, encoding, callback) {
    	this.waiting = false;
    	this.push('u001b[32m' + chunk + 'u001b[39m');
    	callback();
    };
    HungryStream.prototype._read = function (size) {
    	if (!this.waiting) {
    		this.push('Feed me data! > ');  //display 
    		this.waiting = true;
    	}
    };
    var hungryStream = new HungryStream();
    process.stdin.pipe(hungryStream).pipe(process.stdout);
     
    + transform streams are specifically designed for changing data, need implement the _transform method 
     
    + advanced patterns and optimization, read data from a file but concerned about either speed or memory performance 
    
    optimize stream's buffer size to suite your app 
     
    var fs = require('fs');
    var zlib = require('zlib');
    function benchStream(inSize, outSize) {
    	var time = process.hrtime();
    	var watermark = process.memoryUsage().rss;
    	var input = fs.createReadStream('/usr/share/dict/words', {
    			bufferSize : inSize
    		});
    	var gzip = zlib.createGzip({
    			chunkSize : outSize
    		});
    	var output = fs.createWriteStream('out.gz', {
    			bufferSize : inSize
    		});
    	var memoryCheck = setInterval(function () {
    			var rss = process.memoryUsage().rss;
    			if (rss > watermark) {
    				watermark = rss;
    			}
    		}, 50);
    	input.on('end', function () {
    		var memoryEnd = process.memoryUsage();
    		clearInterval(memoryCheck);
    		var diff = process.hrtime(time);
    		console.log([
    				inSize,
    				outSize,
    				(diff[0] * 1e9 + diff[1]) / 1000000,
    				watermark / 1024].join(', '));
    	});
    	input.pipe(gzip).pipe(output);
    	return input;
    }
     
    console.log('file size, gzip size, ms, RSS');
    var fileSize = 128;
    var zipSize = 5024;
    function run(times) {
    	benchStream(fileSize, zipSize).on('end', function () {
    		times--;
    		fileSize *= 2;
    		zipSize *= 2;
    		if (times > 0) {
    			run(times);
    		}
    	});
    }
    run(10);
    ((callout - streams - buffer - size - 8))
    
    + using the old streams api, use stream.Readable class 
     
    + adapting streams based on their destination, make a stream behave differently when it's piped to the TTY 
     
    bind a listener to the pipe event and then use stream.isTTY to check if the stream s bound to a terminal 
    // A simple writable stream
    function OutputStream() {
    	stream.Writable.call(this);
    	this.on('pipe', function (dest) {
    		dest.isTTY = this.isTTY;
    	}
    		.bind(this));
    }
    OutputStream.prototype._write = function (chunk, encoding, cb) {
    	util.print(chunk.toString());
    	cb();
    };
    var memoryStream = new MemoryStream();
    
    + testing streams, use some suitable sample data and then call read() or write 
     
- file system 
    + fs module allws developer to interact with 
    POSIX file i/o prmitives 
    file streaming 
    bulk file i/o 
    file watching 
    
    + asynchronous and synchronous approaches for loading configuration files 
    working with the file descriptors 
    advisory file-locking techniques 
    recursive file operations 
    writing a file database 
    watching for file and directory changes 
    
    + POSXI file I/O wrappers, http://mng.bz/7EKM, wrapper module such as readdir 
    var fs = require('fs');
    fs.readdir('/path/to/dir', function (err, files) {
    	console.log(files); // [ 'fileA', 'fileB', 'fileC', 'dirA', 'etc' ]
    });
    
    rename 
    truncate 
    ftruncate 
    chown 
    fchown 
    lchown 
    chmod 
    fchmod, takes a file descriptor 
    lchmod, does not follow link 
    stat 
    lstat 
    fstat 
    link 
    symlink 
    readlink 
    realpath 
    unlink 
    rmdir 
    mkdir 
    readdir 
    close 
    open 
    utimes 
    futimes 
    fsync 
    write 
    read 
    
    var fs = require('fs');
    var assert = require('assert');
    var fd = fs.openSync('./file.txt', 'w+');
    var writeBuf = new Buffer('some data to write');
    fs.writeSync(fd, writeBuf, 0, writeBuf.length, 0);
    var readBuf = new Buffer(writeBuf.length);
    fs.readSync(fd, readBuf, 0, writeBuf.length, 0);
    assert.equal(writeBuf.toString(), readBuf.toString());
    fs.closeSync(fd);
    
    + streaming, fs provde a streaming API, fs.createWriteStream, fs.createReadSream
    
    + Bulk file I/O, for reading fs.readFile and writing fs.writeFile or appending, fs.appendFile 
    
    it handle a entire file into memory or disk 
    
    + file watching fs.watch, fs.watchFile 
    
    + synchronous alternative, sychronous API should be used when setting up your app 
    var fs = require('fs');
    var http = require('http');
    fs.readFileSync('./output.dat');
    http.createServer(function (req, res) {
    	fs.readFileSync('./output.dat');
    }).listen(3000);
    
    + loading and configuration files,
    
    we could use synchronous fs method 
    
    we could also use require to load json file into memory 
    
    + using a file descriptor, use fs file descriptor method, stdin, stdout, stderr 
    
    fs.writeSync(1, 'logging to stdout');
    
    + working with locking, set up a file-locking mechanism using node's built-ins 
     (https://kernel.org/doc/Documentation/filesystems/mandatory-locking.txt)
    
    Node has no built-in support for locking a file directly (either mandatory or advisory). But advisory locking of files can be done using syscalls such as flock (http://linux.die.net/man/2/flock), which is available in a third-party module (http://github.com/ audehlo/node-fs-ext).
    
    + creating lockfiles with mkdir 
    
    + recursive file operations, use recursion and combine file system primitives 
    fs.readdir/fs.readdirSync 
    fs.stat/fs.statSync, give information about file 

    var fs = require('fs');
    var join = require('path').join;
    exports.findSync = function (nameRe, startPath) {
    	var results = [];
    	function finder(path) {
    		var files = fs.readdirSync(path);
    		for (var i = 0; i < files.length; i++) {
    			var fpath = join(path, files[i]);
    			var stats = fs.statSync(fpath);
    			if (stats.isDirectory())
    				finder(fpath);
    			if (stats.isFile() && nameRe.test(files[i]))
    				results.push(fpath);
    		}
    	}
    	finder(startPath);
    	return results;
    }
    
    + writing a file database using in-memory database with append-only journaling 
    var Database = require('./database');
    var client = new Database('./test.db');
    client.on('load', function () {
    	var foo = client.get('foo');
    	client.set('bar', 'my sweet value', function (err) {
    		if (err)
    			return console.error(err);
    		console.log('write successful');
    	});
    	client.del('baz');
    });
    
    each line in the file is a record and it's a JSON object 
    
    + watch file and directory 
    fs.watch and fs.watchFile 
    
    The preferred is fs.watch, but since it’s inconsistent across platforms, it’s a good idea to test whether it does what you want (and better to have a test suite).
    
- networking 
    + network in node, network layers, packets, sockets 
    
    + networking terminology, TCP, transmission control protocol and UDP user datagram protocol 
    
    network concepts 
    layer 
    http 
    tcp 
    udp 
    socket 
    packet 
    datagram, the udp equivalent of a packet 
    MTU, maximum transimission unit 
    
        * layers 
        application     dns, http, irc 
        transport       tcp, udp 
        network         ip, icmp 
        data link       802.3(ethernet), wifi(ieee 802.11)
        physical        10BASE-5, bluetooth, fiber optics 
        
        tcp packet {receiver port, sender port} <- ip packet{receiver address, sender address} <- ethernet packet {receiver mac, sender mac}

    ip protocol doesn't guarantee data integrity or delivery, so we need a transport layer TCP. if the delivery isn't always required then UDP is preferred 
    
    port number is used to specify the data is relative to which program 
    
    + sockets, the basic unit of a network, historically socket meant the Berkeley Sockets API 
    
    if a application need to use both TCP and UDP then convention is to use the same port number 
    
    in node TCP socket could be create with net moule, UDP is supported by dgram module. other protocol are also supported such as DNS 
    
    + node's networking modules, DNS, TCP, HTTP and encryption
    
        * DNS, domain name system
        * HTTP
        * ENCRYPTION, the term SSL, secure sockets layer, node's tls module is implemented using OpenSSL 
        
        the type of encryption is called public key cryptography
        
    + non blocking networking and thread pools 
    
    BSD socket library libuv can make non-blocking TCP and UDP connections 
    
    (http://nikhilm.github.io/uvbook/networking.html#tcp)
    
    + TCP clients and servers 
    net.createServer 
    server.listen to bind it to a port 
    
    var net = require('net');
    var clients = 0;
    var server = net.createServer(function (client) {
    		clients++;
    		var clientId = clients;
    		console.log('Client connected:', clientId);
    		client.on('end', function () {
    			console.log('Client disconnected:', clientId);
    		});
    		client.write('Welcome client: ' + clientId + 'rn');
    		client.pipe(client);
    	});
    server.listen(8000, function () {
    	console.log('Server started on port 8000');
    });
    
- child process 


# Real-word recipes 
- the web, build leaner and meaner web applications 
- tests, the key to confident code 
- debugging designing for introspection and resolving issues 
- node in production, deploying applications safely 


# Writing modules 
- writing modules, mastering what node is all about 







