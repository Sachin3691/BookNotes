Computer vision with python 3=Saurabh Kapur;Note=Erxin

# Introduce to image processing 
- image shows how image processing can be used to detect tumors

# Image processing library
- Pillow is an open source library that has been forked from the Python Imaging Library (PIL). Pillow is a very good starting point for beginners who want to start with implementing some basic algorithms

$ pip install pillow 

- reading an image 
from PIL import Image 

img = Image.open('image.pn')

img.save('temp.png')

- cropping an image, Region of Interest (ROI). The concept of ROI is sometimes useful when you want to run your algorithm only on a particular part of the image and not the entire image. 
from PIL 

>>> from PIL import Image
>>> dim = (100,100,400,400) #Dimensions of the ROI
>>> crop_img = img.crop(dim)
>>> crop_img.show()

- changing between color spaces 

    + An image can be represented in many different modes (color spaces) such as RGB, CMYK, Grayscale, and YUV.
    
    + Grayscale: This is one of the simplest color spaces both in terms of understanding and storing on a computer. Each pixel value in a grayscale image is a single value between 0 and 255
    
    + RGB, White color in RGB space is written as (255, 255, 255) and black is written as (0, 0, 0). Red, green, and blue are represented by (255, 0, 0), (0, 255, 0), and (0, 0, 255) respectively.
    
    + HSV, This is a cylindrical coordinate system where we project RGB values onto a cylinder. 
    
- image depth, Image depth or the color depth is the number of bits used to represent a color of a pixel. The image depth determines the range of colors an image can have

- To convert an image from RGB color space to grayscale color space, use the L mode. 

- rotated (counter clockwise) as an argument:
>>> from PIL import Image        
>>> rotate_img = img.rotate(90)        
>>> rotate_img.show()

- Skimage, an scikit-image provide more advanced operations as compared to Pillow. suite for enterprise app 

$ pip install -U scikit-image 

    + read 
  >>> from skimage import io        
  >>> img = io.imread("image.png")        
  >>> io.imshow("image.png")        
  >>> io.show()
  
    + Draw module: The draw module has various functions to draw different shapes such as circles, ellipses, and polygons. 
    
- Image derivatives 

image derivative is defined as the change in the pixel value of an image. The rate of change of a function is defined as

lim(h->0) {f(x+h)-f(x)}/h

    + forward difference 
    f(x+1) - f(x)
    
    + backward difference 
    f(x) - f(x-1)
    
    + central difference 
    f(x+1) - f(x-1)
    
Given an image matrix, we can find the derivative using another matrix called mask or kernel. 
derivative masks for forward, backward, and central difference are as follows:
forward [1 -1]
backward [-1 1]
central [1 0 -1]

example of derivative mask 
[1 0 -1]
[1 0 -1]
[1 0 -1]

    + apply the image derivative mask 
Image Matrix IM(m,n)
Derivative mask DM(3,3)

1/3 * sum(Cell(IM[i,i+k]) * Cell(DM[i, i+k]))   k in 0 to 2

- Kernels, we use the terms mask, kernel, and filter interchangeably. What these essentially mean is a square matrix of numbers that is used to compute various properties or characteristics in an image

- Convolution in the context of image processing is defined as the sum of the product of the corresponding elements of a kernel matrix to an image matrix. 

- understanding image filters, types of filtering are as follows 
Gaussian blur 
Median filter 
Dilation and erosion 
Customs filters 
Image thresholding 

- Gaussian blur, one the most used filters in image processing. It uses the Gaussian distribution bell curve defined by the following function

f(x) = 1/(__sigma__ * (2 * PI)^1/2) * e ^(-(x - __mu__)^2/(2*__signma__^2))

Gaussian distribution, the center pixel gets the most weight and its neighboring pixels get lesser weight when performing convolution.
        * * 
      *     *
    *         *
* *             * * 

>>> from PIL import Image        
>>> from PIL import ImageFilter        
>>> img = Image.open("image.png")        
>>> blur_img = img.filter(ImageFilter.GaussianBlur(5))        
>>> blur_img.show()

- Median filter,  returns the median value from the pixel and its neighbors
>>> from PIL import Image
>>> from PIL import ImageFilter
>>> img = Image.open("image.png")
>>> blur_img = img.filter(ImageFilter.MedianFilter(7))
>>> blur_img.show()

- Dilation and erosion, Morphological operations on images are operations that use the inherent structure or features of an image and processes the image while maintaining the overall structure. 

    + Erosion, like in geology, means the removal of the top layer of soil or earth by wind, water, and so on. In image processing too, it means to remove parts of the image. Like the top layer of the soil starts depleting, applying erosion to an image makes the objects in the image to shrink while maintaining the overall structure and shape of the image. 
    
        * example, Consider a scenario where you have two objects in an image and they are really close by, and you do want your algorithm to assume that they are the same object. Hence you shrink both the objects to mark a clear distinction between the two objects

        * Skimage provides a binary_erosion() function for erosion in its morphology module.
    
from skimage import morphology
from skimage import io

img = io.imread('image.png')
eroded_img = morphology.binary_erosion(img)

io.imshow(eroded_img)
io.show()

- Dilation, Dilation is just the opposite of erosion. While in erosion we shrunk parts of the image, here we try to expand the parts of the image. 

Skimage provides a binary_dilation() function for dilation in its morphology module.
from skimage import morphology
from skimage import io
img = io.imread('image.png')
dilated_img = morphology.binary_dilation(img)
io.imshow(dilated_img)
io.show()

- Custom filters
The good news is you do not have to write the entire convolution process again from scratch. Skimage and pillow both provide the option of applying custom filters on images

>>> from PIL import ImageFilter>>> kernel = ImageFilter.Kernel((3,3), [1,2,3,4,5,6,7,8,9])

The kernel function takes the size, the sequence of kernel weights

>>> from PIL import Image
>>> from PIL import ImageFilter
>>> img = Image.open("image.png")
>>> img = img.convert("L")
>>> new_img = img.filter(ImageFilter.Kernel((3,3),[1,0,-1,5,0,-5,1,0,1]))
>>> new_img.show()

- Image thresholding 

update the color value of a pixel to either white or black according to a threshold value.

from skimage.filters 
import threshold_otsu, threshold_adaptive
from skimage.io import imread, imsave
from skimage.color import rgb2gray
img = imread('image.jpg')
img = rgb2gray(img)
thresh_value = threshold_otsu(img)
thresh_img = img > thresh_value

- Edge detection 
    + using an image derivative find pixels where there is change in the pixel values among the neighboring pixels and that pixel will probably be a port of an edge 

- Sobel edge detector, The idea behind the Sobel edge detector is to find the pixels with a large magnitude of gradient values.

delta f = (delta f)/ (delta x) * i + (delta f)/(delta y) * j + (delta f)/(delta z) * k 

    + kernel used in sobel edge detector algorithm 
    x filter 
    -1 0 1 
    -2 0 2 
    -1 0 1 
    
    y filter 
    1 2 1 
    0 0 0 
    -1 -2 -1
    
>>> from skimage import io
>>> from skimage import filters
>>> from skimage import color
>>> img = io.imread("image.png")
>>> img = color.rgb2gray(img)
>>> edge = filters.sobel(img)
>>> io.imshow(edge)
>>> io.show()

- Canny edge detector is another very important algorithm. It also uses the concept of gradients like in the Sobel edge detector, but in Sobel we only considered the magnitude of the gradient. In this we will also use the direction of the gradient to find the edges.

how four major steps 
    + smoothing, use the gausssian filter is applied to the image to reduce the noise in the image 
    + finding the gradient, after removing the noise, the next step is to find the gradient magnitude and direction by calculating the x derivative and y derivateive 
    
    if we know the direction of the gradient we can find the direction of the edges as well 
    
    + nomaximal suppression, check whether the gradient calculated is maximum among the neighboring points lying in the positive and negative direction of the gradient. That is whether it is the local maxima in the direction of the gradient 
    
    (x3,y3) -- (x2, yt2) -->(x1, y1)
    
>>> from skimage import io
>>> from skimage import feature
>>> from skimage import color
>>> img = io.imread("image.png")
>>> img = color.rgb2gray(img)
>>> edge = feature.canny(img,3)
>>> io.imshow(edge)>>> io.show()

- hough line , exploit the same concept and try to find the slope and constants of the lines (if any) in an image. Given any two points in the image, we substitute in the equation of the line (as shown next) and solve for the slope and intercept of the line. For example, let the two given points be (x1, y1), (x2, y2).

 Hough transform. Hough transformation is a general framework that takes in the parameterized equations of rigid shapes and detects these shapes in an image.

y = mx + c 
y1 = mx1 + c 
y2 = mx2 + c 

from skimage.transform import (hough_line, probabilistic_hough_line)
from skimage.feature import canny

image = io.imread('image.png')

edges = canny(image, 2, 1, 25)
lines = hough_line(image)
probabilistic_lines = probabilistic_hough_line(edges, threshold=10, line_length=5, line_gap=3)

- Hough circle, we use the following equation of a circle, where (h, k) is the center of the circle and r is the radius

(x - h)^2 + (y - k)^2 = r^2 


# Revisiting image features 
- we essentially want to describe an image in terms of its features. 

    + corner detection 
    + cascade classifier 
    + oriented fast and rotated BRIEF (ORB)
    
- revisiting image features, we use more sophisticated feature descriptors such as corners, Local Binary Pattern (LBP), BRISK, and Oriented FAST and Rotated BRIEF. Let's understand what is different in these feature descriptors
    
- he Harris corner detector; a corner is defined as a point where the image changes significantly in all directions

sigma[I(x + u, y + v) - I(x, y)]^2 
    
from matplotlib import pyplot as plt
from skimage.io import 
from matplotlib import pyplot as plt
from skimage.io import imread
from skimage.color import rgb2gray
from skimage.feature import corner_harris, corner_subpix, corner_peaks

image = imread('test.png')
image = rgb2gray(image)
corners = corner_harris(image)
coords = corner_peaks(corners, min_distance=5)
coords_subpix = corner_subpix(image, coords, window_size=13)
fig, ax = plt.subplots()

ax.imshow(image, interpolation='nearest', cmap=plt.cm.gray)
ax.plot(coords[:, 1], coords[:, 0], '.b', markersize=3)
ax.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15)
ax.axis((0, 350, 350, 0))
plt.show()
    
- local binary patterns, The Local Binary Patterns (LBP) cascade is a type of cascade classifier 
    
LBP, an eight-bit binary feature vector is created for each pixel in the image by considering the eight neighboring pixels (top-left, top-right, left, right, bottom-left, and bottom-right). For every neighboring pixel, there is a corresponding bit, which is assigned a value 1 if the pixel value is greater than the center pixel's value, otherwise it is 0.

    + spot, all greater than center 
    + spot/flat  all less than center
    + line end, most of points large than center 
    + edge, half of points large than center 
    + corner, similar to corner 

from skimage.transform import rotate
from skimage.feature import local_binary_pattern
from skimage import data
from skimage.color import label2rgb
import numpy as np

brick = data.load('brick.png')
grass = data.load('grass.png')
wall = data.load('rough-wall.png')

brick_lbp = local_binary_pattern(brick, 16, 2, 'uniform')
grass_lbp = local_binary_pattern(grass, 16, 2, 'uniform')
wall_lbp = local_binary_pattern(wall, 16, 2, 'uniform')

brick_rot = rotate(brick, angle = 22, resize = False)
grass_rot = rotate(grass, angle = 22, resize = False)
wall_rot = rotate(wall, angle = 22, resize = False)

brick_rot_lbp = local_binary_pattern(brick_rot, 16, 2, 'uniform')
grass_rot_lbp = local_binary_pattern(grass_rot, 16, 2, 'uniform')
wall_rot_lbp = local_binary_pattern(wall_rot, 16, 2, 'uniform')

bins_num = int(brick_lbp.max() + 1)
brick_hist = np.histogram(brick_lbp, normed=True, bins=bins_num, range=(0, bins_num))

lbp_features = [brick_rot_lbp, grass_rot_lbp, wall_rot_lbp]
min_score = 1000 # Set a very large best score value initially
idx = 0 # To keep track of the winner

for feature in lbp_features:
    histogram, _ = np.histogram(feature, normed=True, bins=bins_num, range=(0,bins_num))
    p = np.asarray(brick_hist)
    q = np.asarray(histogram)
    filter_idx = np.logical_and(p != 0, q != 0)
    score = np.sum(p[filter_idx] * np.log2(p[filter_idx] / q[filter_idx]))
    if score < min_score:
        min_score = score
        winner = idx
    idx = idx + 1

if idx == 0:
    print('Brick matched with Brick Rotated')
elif idx == 1:
    print('Brick matched with Grass Rotated')
elif idx == 2:
    print('Brick matched with Wall Rotated')
    
- Oriented FAST and Rotated BRIEF (ORB) was developed at OpenCV labs by Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski in 2011, as an efficient and viable alternative to SIFT and SURF. 
    
- FAST detector, We use a ring radius of nine pixels as it gives good performance

m_p_q = sigma_(x,y) x^p y^p I_(x, y)
C = |m_10, m01|
    |m_0, m_00|
__theta__ = atan2(m_01, m_10)
Here, atan2 is the quadrant-aware version of arc tan. To improve the rotation in-variance of this measure, we make sure that the moments are computed with x and y remaining within a circular region of radius r. 
    
- ORB specifies the rBRIEF algorithm as follows
Run each test against all training patches.
Order the tests by their distance from a mean of 0.5, forming the vector T.
Perform a greedy search:
    + Put the first test into the result vector R and remove it from T.
    + Take the next test from T and compare it against all tests in R. 
    + If its absolute correlation is greater than a threshold, discard it; otherwise add it to R.
    + Repeat the previous step until there are 256 tests in R. If there are fewer than 256, raise the threshold and try again
    
from skimage import data
from skimage import transform as tf
from skimage.feature import (match_descriptors, corner_harris,
 corner_peaks, ORB, plot_matches)
from skimage.color import rgb2gray
import matplotlib.pyplot as plt

#Read the original image
image_org = data.astronaut()

#Convert the image gray scale
image_org = rgb2gray(image_org)

#We prepare another image by rotating it. Only to demonstrate feature mathcing
image_rot = tf.rotate(image_org, 180)

#We create another image by applying affine transform on the image
tform = tf.AffineTransform(scale=(1.3, 1.1), rotation=0.5,
 translation=(0, -200))
image_aff = tf.warp(image_org, tform)

#We initialize ORB feature descriptor
descriptor_extractor = ORB(n_keypoints=200)

#We first extract features from the original image
descriptor_extractor.detect_and_extract(image_org)
keypoints_org = descriptor_extractor.keypoints
descriptors_org = descriptor_extractor.descriptors

descriptor_extractor.detect_and_extract(image_rot)
keypoints_rot = descriptor_extractor.keypoints
descriptors_rot = descriptor_extractor.descriptors

descriptor_extractor.detect_and_extract(image_aff)
keypoints_aff = descriptor_extractor.keypoints
descriptors_aff = descriptor_extractor.descriptors

matches_org_rot = match_descriptors(descriptors_org, descriptors_rot, cross_check=True)
matches_org_aff = match_descriptors(descriptors_org, descriptors_aff, cross_check=True)

fig, ax = plt.subplots(nrows=2, ncols=1)

plt.gray()

plot_matches(ax[0], image_org, image_rot, keypoints_org, keypoints_rot, matches_org_rot)
ax[0].axis('off')
ax[0].set_title("Original Image vs. Transformed Image")

plot_matches(ax[1], image_org, image_aff, keypoints_org, keypoints_aff, matches_org_aff)
ax[1].axis('off')
ax[1].set_title("Original Image vs. Transformed Image")

plt.show()

- image stitch,stitches the two images together and outputs a combined image

from skimage.feature import ORB, match_descriptors
from skimage.io import imread
from skimage.measure import ransac
from skimage.transform import ProjectiveTransform
from skimage.color import rgb2gray
from skimage.io import imsave, show
from skimage.color import gray2rgb
from skimage.exposure import rescale_intensity
from skimage.transform import warp
from skimage.transform import SimilarityTransform
import numpy as np


image0 = imread('goldengate1.png')
image0 = rgb2gray(image0)

image1 = imread('goldengate2.png')
image1 = rgb2gray(image1)

orb = ORB(n_keypoints=1000, fast_threshold=0.05)

orb.detect_and_extract(image0)
keypoints1 = orb.keypoints
descriptors1 = orb.descriptors

orb.detect_and_extract(image1)
keypoints2 = orb.keypoints
descriptors2 = orb.descriptors

matches12 = match_descriptors(descriptors1,
 descriptors2,
 cross_check=True)

src = keypoints2[matches12[:, 1]][:, ::-1]
dst = keypoints1[matches12[:, 0]][:, ::-1]

transform_model, inliers = \
 ransac((src, dst), ProjectiveTransform, min_samples=4, residual_threshold=2)

r, c = image1.shape[:2]

corners = np.array([[0, 0],
 [0, r],
 [c, 0],
 [c, r]])

warped_corners = transform_model(corners)

all_corners = np.vstack((warped_corners, corners))

corner_min = np.min(all_corners, axis=0)
corner_max = np.max(all_corners, axis=0)

output_shape = (corner_max - corner_min)
output_shape = np.ceil(output_shape[::-1])

offset = SimilarityTransform(translation=-corner_min)

image0_warp = warp(image0, offset.inverse, output_shape=output_shape, cval=-1)

image1_warp = warp(image1, (model_robust + offset).inverse, output_shape=output_shape, cval=-1)

image0_mask = (image0_warp != -1)
image0_warp[~image0_mask] = 0
image0_alpha = np.dstack((gray2rgb(image0_warp), image0_mask))

image1_mask = (image1_warp != -1)
image1_warp[~image1_mask] = 0
image1_alpha = np.dstack((gray2rgb(image1_warp), image1_mask))

merged = (image0_alpha + image1_alpha)
alpha = merged[..., 3].
merged /= np.maximum(alpha, 1)[..., np.newaxis]

imsave('output.jpg', merged)
    
    
# Understand images better 
- techniques in this chapter 
    + contour detection 
    + watershed algorithm 
    + superpixels 
    + graph cut 
- segmentation, On the left you see the original image and on the right there is the corresponding segmented image. As we can see, the algorithm was successful in grouping together similar parts of the image. Like the entire background with bushes was grouped to dark green color. 

- contours. Simply put, contours are nothing but boundaries of objects in an image. Say, for example, you have different types of bottles in an image and you want to segment out each one of them

    + convert the image to grayscale and run a sobel edge detection over it
    
from skimage import measure
from skimage.io import imread
from skimage.color import rgb2gray
from skimage.filters import sobel
import matplotlib.pyplot as plt

#Read an image
img = imread('contours.png')

#Convert the image to grayscale
img_gray = rgb2gray(img)

#Find edges in the image
img_edges = sobel(img_gray)

#Find contours in the image
contours = measure.find_contours(img_edges, 0.2)

# Display the image and plot all contours found

fig, ax = plt.subplots()
ax.imshow(img_edges, interpolation='nearest', cmap=plt.cm.gray)

for n, contour in enumerate(contours):
    ax.plot(contour[:, 1], contour[:, 0], linewidth=2)

ax.axis('image')
ax.set_xticks([])
ax.set_yticks([])

plt.show()

- watershed algorithm, To begin with, imagine that your image is the surface that we are trying to segment. All the objects, background, and foreground in the image are craters. As we know, our next task is to identify the center of each of these craters (essentially objects, background, and foreground). Here is the trick—in images, it is not so easy to identify the center of the objects because if we knew what our object was, I probably would not have written this chapter! So how do we solve this problem? We find out the local gradients of the image. By doing so, we will identify all the local minimums in our images. These local minimums will give us an approximate idea of where the objects could possibly be located. In technical terms, these local minimums are called markers. We assign each marker with a unique color and then start filling these colors until we reach the boundary of an adjacent marker.

    + algorithm is as follows:
    Read the image that you want to segment.
    Convert it into grayscale (only if it is not in grayscale already).
    Convert the image pixel values to unsigned int using the img_as_ubyte() function. This is because the gradient function expects the image in a certain format.
    Calculate the local gradients of the image.
    Apply the Watershed algorithm

from scipy import ndimage as ndi
from skimage.morphology import watershed, disk
from skimage import data
from skimage.io import imread
from skimage.filters import rank
from skimage.color import rgb2gray
from skimage.util import img_as_ubyte

img = data.astronaut()
img_gray = rgb2gray(img)

image = img_as_ubyte(img_gray)

#Calculate the local gradients of the image
#and only select the points that have a
#gradient value of less than 20
markers = rank.gradient(image, disk(5)) < 20
markers = ndi.label(markers)[0]

gradient = rank.gradient(image, disk(2))
#Watershed Algorithm
labels = watershed(gradient, markers)

- superpixels,  You do not always want to iterate through all the pixels in the image. As an attempt to remove redundancy in the pixels of an image, we try to combine pixels closer to each other that have the same color value into a cluster and then call those clusters superpixels.

from skimage import segmentation, color
from skimage.io import imread
from skimage.future import graph
from matplotlib import pyplot as plt

img = imread('test.jpeg')
img_segments = segmentation.slic(img, compactness=20, n_segments=500)
superpixels = color.label2rgb(img_segments, img, kind='avg')

- normalized graph cut, The simplest explanation of the graph cut technique is that each pixel in the image is treated as a node. Apart from these nodes, we have some extra nodes that each represent, say an object in the image. All the pixels are connected to all of its adjacent pixels and each to the object nodes

How the graph cut algorithm works--an image graph is created and using it the seed cuts are made in the graph. As a result, we get a well-segmented image

    + reference, The normalized cuts technique was published in the paper: Shi, J.; Malik, J.,“Normalized cuts and image segmentation”, Pattern Analysis and Machine Intelligence, IEEE Transactions, vol. 22, no. 8, pp. 888-905, August 2000
    
    + steps 
Read the image.
Perform k-means clustering over color values. In our implementation, we use the SLIC method for clustering.
Using the clustered pixels from the previous step, we create a weighted graph over these clusters. The weight of each edge is determined by how similar two regions are.
We apply the normalized graph cut technique over the graph obtained in the last step.

from skimage import data, segmentation, color
from skimage.io import imread
from skimage import data
from skimage.future import graph

img = data.astronaut()
img_segments = segmentation.slic(img, compactness=30, n_segments=200)
out1 = color.label2rgb(img_segments, img, kind='avg')

segment_graph = graph.rag_mean_color(img, img_segments, mode='similarity')
img_cuts = graph.cut_normalized(img_segments, segment_graph)
normalized_cut_segments = color.label2rgb(img_cuts, img, kind='avg')


# Integrating machine learning with computer vision 
- sklearn, machine learning, 
k-means claustering 
logistic regression 
support vector machines 

- This is one of the most common applications of machine learning in computer vision

- logistic regression, logistic regression tries to fit a curve between the data points that best represent it. A more formal definition is; it is a technique that finds relationships between a set of independent variables and a dependent variable—here, the independent variables being the input data and the dependent variable being the labels corresponding to the data.
    + example for recognize the number 0 to 9 

from sklearn import datasets, metrics
from sklearn.linear_model import LogisticRegression


mnist = datasets.load_digits()

images = mnist.images

data_size = len(images)

images = images.reshape(len(images), -1)
labels = mnist.target

LR_classifier = LogisticRegression(C=0.01, penalty='l1', tol=0.01)

LR_classifier.fit(images[:int((data_size / 4) * 3)], labels[:int((data_size / 4) * 3)])

predictions = LR_classifier.predict(images[int((data_size / 4)):])
target = labels[int((data_size/4)):]

print("Performance Report: \n %s \n" % (metrics.classification_report(target, predictions)))

    + how to use a custom image and make the logistic regression predict the digit

from sklearn import datasets, metrics
from sklearn.linear_model import LogisticRegression

mnist = datasets.load_digits()

images = mnist.images
data_size = len(images)

images = images.reshape(len(images), -1)
labels = mnist.target

LR_classifier = LogisticRegression(C=0.01, penalty='l1', tol=0.01)

LR_classifier.fit(images[:int((data_size / 4) * 3)], labels[:int((data_size / 4) * 3)])

predictions = LR_classifier.predict(images[int((data_size / 4)):])
target = labels[int((data_size/4)):]

print("Performance Report: \n %s \n" % (metrics.classification_report(target, predictions)))

    + use a customized image and predict the output 
from sklearn import datasets, metrics
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from skimage import io, color, feature, transform

mnist = datasets.load_digits()
images = mnist.images
data_size = len(images)

images = images.reshape(len(images), -1)
labels = mnist.target

LR_classifier = LogisticRegression(C=0.01, penalty='l1', tol=0.01)

LR_classifier.fit(images[:int((data_size / 4) * 3)], labels[:int((data_size / 4) * 3)])

digit_img = io.imread('digit.png')
digit_img = color.rgb2gray(digit_img)

digit_img = transform.resize(digit_img, (8, 8), mode="wrap")

digit_edge = feature.canny(digit_img, sigma=5)
digit_edge = digit_edge.flatten()

prediction = LR_classifier.predict(digit_edge)
print(prediction)

- support vector machines, fit a curve that passes through the data points, but in SVMs we try to find hyperplanes that divide the given data into regions 

Hyperplanes (green lines) divide the dataset into appropriate regions
from sklearn import datasets, metrics, svm

mnist = datasets.load_digits()

images = mnist.images

data_size = len(images)

images = images.reshape(len(images), -1)
labels = mnist.target

SVM_classifier = svm.SVC(gamma=0.001)

Support Vector Machine
SVM_classifier.fit(images[:int((data_size / 4) * 3)], labels[:int((data_size / 4) * 3)])

predictions = SVM_classifier.predict(images[int((data_size / 4)):])
target = labels[int((data_size/4)):]

print("Performance Report: \n %s \n" % (metrics.classification_report(target, predictions)))

The svm.SVC function provides an option to select the kind of hyperplanes that we want to compute. The kernel parameter in the function can take values such as linear, poly, rbf, or sigmoid. By default, it uses rbf. These different kernel choices help the user in finding the most appropriate kernel for the kind of data that they are dealing with.

    + reduced dimension data 
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, decomposition

digits = datasets.load_digits(n_class=6)
X = digits.data
y = digits.target
n_samples, n_features = X.shape
n_neighbors = 30

X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)

fig, plot = plt.subplots()
plot.scatter(X_pca[:, 0], X_pca[:, 1])
plot.set_xticks(())
plot.set_yticks(())

- k-means clustering, The most common form of a similarity metric is the distance between two points in the given space. Points closer to each other are clustered together. Initially, when the algorithm begins, we randomly select k points that represent the center of each of the k clusters. Then, iteratively, we keep updating these k center points such that they form the mean of the final k clusters.

from sklearn import datasets, metrics
from sklearn.cluster import KMeans

mnist = datasets.load_digits()
images = mnist.images
data_size = len(images)

images = images.reshape(len(images), -1)
labels = mnist.target

clustering = KMeans(n_clusters=10, init='k-means++', n_init=10)

clustering.fit(images[:int((data_size / 4) * 3)])

print(clustering.labels_)

predictions = clustering.predict(images[int((data_size / 4)):])


# Image classification using neural networks 
- contents 
    + introduce to neural networks 
    + convolutional neural networks 
    + challenges in machine learning 

This is called a fully connected neural network. We will see in this chapter, some other types of networks that are more commonly used in computer vision these days

- MNIST digit classification, steps 
Preprocess the dataset by normalizing the pixel values of the images between 0, 1 or -1, and 1 (to make the mean 0).
Prepare the dataset. Split the dataset into two sets—training set and testing set.
Start training the dataset over the test data.
Compute your network's performance over the test dataset.


from sklearn.datasets import fetch_mldata
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split

print('Getting MNIST Data...')
mnist = fetch_mldata('MNIST original')
print('MNIST Data downloaded!')

images = mnist.data 
labels = mnist.target 

images = normalize(images, norm='l2') #You can use l1 norm too

images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.25, random_state=17)

nn = MLPClassifier(hidden_layer_sizes=(100), max_iter=20, solver='sgd', learning_rate_init=0.001, verbose=True)

print('NN Training started...')
nn.fit(images_train, labels_train)
print('NN Training completed!')

print('Network Performance: %f' % nn.score(images_test, labels_test))

- convolutional neural networks, CNNs are a variant of the traditional neural networks, where unlike the neural networks, not all perceptrons are connected to each other. In CNNs, the connections between perceptrons are sparse. Apart from that, each layer in a CNN can behave in a different manner. Let's take an example of a basic CNN and use that as a reference to explain the different concepts involved. The architecture that we will look at is called LeNet

Convolutions -> subsampling convolutions-> subsampling full connection -> full connection -> gausssian connections 

    + reference 
http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

    + we know what CNNs are, let's try to implement LeNet using the sklearn library.

from keras.models import Sequential
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Activation, Flatten, Dense
from keras.optmizers import SGD
from keras.utils import np_utils
from sklearn import datasets

num_classes=9
img_depth=1
img_height=28
img_width=28

model = Sequential()

model.add(Convolution2D(20, 5, 5, border_mode="same", input_shape=(img_depth, img_height, img_width)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Convolution2D(50, 5, 5, border_mode="same"))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

model.add(Flatten())
model.add(Dense(500))

model.add(Activation("relu"))

model.add(Dense(num_classes))
model.add(Activation("softmax"))

mnist = datasets.fetch_mldata("MNIST Original")

mnist.data = mnist.data.reshape((mnist.data.shape[0], 28, 28))
mnist.data = mnist.data[:, np.newaxis, :, :]
mnist.data = mnist.data / 255.0 #Normalize the images to [0, 1.0]

train_data, test_data, train_label, test_label = train_test_split(minist.data, mnist.target, test_size=0.25)
train_label = np_utils.to_categorical(train_label, 10)
test_label = np_utils.to_categorical(test_label, 10)

model.compile(loss="categorical_crossentropy", optimizer=SGD(lr=0.0001), metrics=["accuracy"])

model.fit(train_data, train_label, batch_size=32, no_epoch=30, verbose=1)

loss, accuracy = model.evaluate(test_data, test_label, batch_size=64, verbose=1)
print("Accuracy: %".format(accuracy * 100))

    + This is one of the most simple CNNs. The more advanced CNN architectures such as ResNet and Inception are way more accurate and powerful in tasks of image classification. Implementing these architectures are beyond the scope of this book


# Introduction to computer vision using OpenCV 
- include 
    + morphological operations 
    + edge detection 
    + contour detection 
    + filters 
    + template matching 

- Writing/saving the image 
>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> cv2.imwrite("saved_image.jpg", img)

- Changing the color space 
COLOR_BGR2GRAY
COLOR_BGR2HSV
COLOR_HSV2BGR
COLOR_BGR2YUV
COLOR_GRAY2BGR 

>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
>>> cv2.imwrite("gray_image.jpg", gray)
>>> cv2.imshow("image",gray)

- Scaling, used to resize the image 
cv2.INTER_AREA: This algorithm is preferred for shrinking the image
cv2.INTER_CUBIC: This algorithm is preferred for zooming (slow)
cv2.INTER_LINEAR: This algorithm is preferred for zooming
cv2.INTER_LINEAR: This is the default algorithm


>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> r,c = img.shape[:2]
>>> new_img = cv2.resize(img, (2*r,2*c), interpolation = cv2.INTER_CUBIC)
>>> cv2.imwrite("resize_image.jpg", new_img)
>>> cv2.imshow("resize", new_img)

- Cropping the image 
>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> img_crop = img[0:200, 150:350]
>>> cv2.imwrite("crop_img.jpg", img_crop)
>>> cv2.imshow("crop", img_crop)

- Translation, The transformation matrix for the translation is as follows

M = [1 0 t_x]
    [0 1 t_y]
    
>>> import cv2
>>> import numpy as np
>>> img = cv2.imread("image.jpg")
>>> r,c = img.shape[:2]
>>> M = np.float32([[1,0,100],[0,1,100]])
>>> new_img = cv2.warpAffine(img,M,(c,r))
>>> cv2.imwrite("translation.jpg", new_img)
>>> cv2.imshow("translation", new_img)

- rotation 
M = [cos__theta__ -sin__theta__]
    [sin__theta__ cos__theta__]

>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> r,c = img.shape[:2]
>>> M = cv2.getRotationMatrix2D((c/2,r/2),90,1)
>>> new_img = cv2.warpAffine(img,M,(c,r))
>>> cv2.imwrite("rotate_img.jpg", new_img)
>>> cv2.imshow("rotate", new_img)


- Thresholding, OpenCV has an inbuilt threshold() function, which takes a grayscale image, threshold value, and new value to be assigned if the value is greater than the threshold and type of thresholding as input

cv2.THRESH_BINARY
cv2.THRESH_BINARY_INV
cv2.THRESH_TRUNC
cv2.THRESH_TOZERO
cv2.THRESH_TOZERO_INV

>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
>>> new_img = cv2.threshold(gray,120,255,cv2.THRESH_BINARY)
>>> cv2.imwrite("thresholding.jpg", new_img[1])
>>> cv2.imshow("thresholding", new_img[1])

- filters are created by convolution of a kernel with an image. To do this operation, OpenCV provides the cv2.filter2D() function, which takes the image, destination image depth, and kernel as input. Using this we can create our own filter.

>>> import cv2
>>> import numpy as np
>>> img = cv2.imread("image.jpg")
>>> ker = np.array([[1, 1, 1],
... [1, 1, 1],
... [1, 1, 1]])
>>> new_img = cv2.filter2D(img,-1,ker)
>>> cv2.imwrite("filter.jpg", new_img)

- Gaussian blur, deviation is calculated from the size of the kernel 

>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> new_img = cv2.GaussianBlur(img,(5,5),0)
>>> cv2.imwrite("gaussian_blur.jpg", new_img)
>>> cv2.imshow("gaussian_blur.jpg", new_img)

- median blur 
>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> new_img = cv2.medianBlur(img,5)
>>> cv2.imwrite("median_blur.jpg", new_img)
>>> cv2.imshow("median_blur", new_img)

- Mophological operations, the erode() function in OpenCV requries an image, kernel and the number of times the erosion should be applied as input 

    + Erosion

>>> import cv2
>>> import numpy as np
>>> img = cv2.imread("thresholding.jpg")
>>> ker = np.ones((5,5),np.uint8)
>>> new_img = cv2.erode(img,ker,iterations = 1)
>>> cv2.imwrite("erosion.jpg", new_img)
>>> cv2.imshow("erosion", new_img)

    + Dilation

>>> import cv2
>>> import numpy as np
>>> img = cv2.imread("thresholding.jpg")
>>> ker = np.ones((5,5),np.uint8)
>>> new_img = cv2.dilate(img,ker,iterations = 1)
>>> cv2.imwrite("dilation.jpg", new_img)
>>> cv2.imshow("dilation", new_img)

- edge detection 
    + sobel edge detection, The Sobel() function takes the image, output image depth, order of derivative in x and y direction, and the size of the kernel as input. 

>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
>>> x_edges = cv2.Sobel(gray,-1,1,0,ksize=5)
>>> cv2.imwrite("sobel_edges_x.jpg", x_edges)
>>> y_edges = cv2.Sobel(gray,-1,0,1,ksize=5)
>>> cv2.imwrite("sobel_edges_y.jpg", y_edges)
>>> cv2.imshow("xedges", x_edges)
>>> cv2.imshow("yedges", y_edges)

    + canny edge detector 
>>> import cv2
>>> img = cv2.imread("image.jpg")
>>> gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
>>> edges = cv2.Canny(gray, 100, 200, 3)
>>> cv2.imwrite("canny_edges.jpg", edges)
>>> cv2.imshow("canny_edges", edges)

- contour detection, first is the input image array, the second is the contours found in the image, and the third is the hierarchy array. The hierarchy array stores the relation between the contours

contour retrieval model tells us about the kinds of hierarchy of contours. RETR_LIST they are considered to be in 

>>> import cv2
>>> img = cv2.imread('image.jpg')
>>> gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
>>> thresh_img = cv2.threshold(gray,127,255,0)
>>> im, contours, hierarchy = cv2.findContours(thresh_img[1],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
>>> cv2.drawContours(img, contours, -1, (255,0,0), 3)
>>> cv2.imwrite("contours.jpg", img)
>>> cv2.imshow("contours", img)

- template matching 

 how to locate a template image in an image using some OpenCV functions. A smaller image template matching will help get coordinates in a larger image that match with the template.

cv2.TM_CCOEFF
cv2.TM_CCOEFF_NORMED
cv2.TM_CCORR
cv2.TM_CCORR_NORMED
cv2.TM_SQDIFF
cv2.TM_SQDIFF_NORMED

import cv2

img = cv2.imread("image.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

img_temp = cv2.imread("template.jpg")
gray_temp = cv2.cvtColor(img_temp, cv2.COLOR_BGR2GRAY)

w, h = gray_temp.shape[::-1]

output = cv2.matchTemplate(gray,gray_temp,cv2.TM_CCOEFF_NORMED)
min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(output)

top = max_loc
bottom = (top[0] + w, top[1] + h)

cv2.rectangle(img,top, bottom, 255, 2)

cv2.imshow("image",img)
cv2.imwrite("img.jpg",img)


# Object detection using opencv 
- object detection methods provided in opencv 
    + cascade calssifier, haar cascades 
    + scale invariant feature transformation(SIFT)
    + speeded up robust features(SURF)

- Haar Cascades is a type of cascade classifier, where the system is made up of multiple chained classifiers

extract Haar features from the images using convolutional kernels. Feature values are obtained by subtracting the sum of white pixels under the white rectangle from the sum of pixels under the black rectangle. We slide these kernels (nothing but Haar features) over the entire image and calculate the feature values. 
+------+        
|      | white  
|------|
|******| black 
+------+

+------+
|  | * | edge feature 
|  | * | 
+--+---+ 

+------+        
|      | 
+------|
|******| line feature 
|------|
|      | 
+------+

+--+--+
|  |**|
+--+--+ four rectangle feature 
|**|  | 
+--+--+

- Integral images, also known as summed area tables, are another form of storing images. 

I_sigma(x, y) = __Sigma__ i(x', y')  x' <= x, y' <= y 

For computing the value for pixel (5, 5), we compute the value for pixel (1, 1), (2, 2), and so forth. We can avoid these calculations if we use the precomputed values in the integral image.

import cv2

face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')

img = cv2.imread('image.jpg')
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

faces = face_cascade.detectMultiScale(img_gray, 1.3, 5) 

for (x,y,w,h) in faces:
    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    roi_gray = img_gray[y:y+h, x:x+w]
    roi_color = img[y:y+h, x:x+w]
    eyes = eye_cascade.detectMultiScale(roi_gray)
    for (ex,ey,ew,eh) in eyes:
        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)

cv2.imwrite('output.jpg',img)

reduce computation cost, we run the eye detection only in the face region by extracting the region of interest(ROI)

in face detection, first find the face then use the face area to find the eyes 

- Scale invariant feature transformation(SIFT)
Its scale, translation, and rotation invariance, its robustness to change in contrast, brightness, and other transformations

    + reference original paper 
http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf

    + properties of SIFT 
It is invariant to scaling and rotation changes in objects
It is also partially invariant to 3D viewpoints and illumination changes
A large number of keypoints (features) can be extracted from a single image

    + algorithm is divided into following main stages 
Scale-space extrema detection
Keypoint localization
Orientation assignment
Keypoint descriptor

    + scale-space extrema detection 
achieve scale invariance by generating a multiple scale pyramid of the original images 

essentially doing here is getting rid of details that do not persist at different scales. By doing so, we are only left with information that is scale invariant. Now, to achieve the pyramid 

for example use different sigma to calculate Gaussian Blur etc 

Here, D represents the Difference of Gaussian, G represents the Gaussian filter, L is the Laplacian of Gaussian

To generate the Laplacian of Gaussian images, we calculate the difference between two consecutive images in an octave. This is called the Difference of Gaussian (DoG).

    + keypoint localization, for SIFT, we will not only look at these eight pixels but also at the nine pixels in the preceding images and below this image in the scale space or octave

SIFT recommends that you generate two such extrema images. Thus, to generate two extrema, we need four DoG images. 

 To filter these points out, we use an approach that is similar to the one used in the Harris corner detector 

To eliminate keypoints along the edges, we calculate two gradients at the keypoint, which are perpendicular to each other. The region around the keypoint can be one of the following three types

A region (both gradients will be small).
An edge (here, the gradient parallel to the edge will be small, but the one perpendicular to it will be large).
A corner (both gradients will be large) we want only corners as our keypoints

we again need to use the Taylor series expansion to get the intensity value at subpixel locations

    + orientation assignment, We try to compute the magnitude and direction of the Gaussian blurred images for each keypoint. The magnitudes and directions are calculated using these formula 
    
    + keypoint decriptor, generate a descriptor, we take a 16 x 16 window around the keypoint and break it into 16 windows of size 4 x 4. This can be seen in the following figure

    + example SIFT in OpenCV to find keypoint 
import cv2

image = cv2.imread('image.jpg')
gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
sift_obj = cv2.xfeatures2d.SIFT_create()
keypoints = sift_obj.detect(gray,None)
img=cv2.drawKeypoints(gray,keypoints,image)
cv2.imwrite('sift_keypoints.jpg',image)

    + example to match keypoint in different images 
import cv2
import random

image = cv2.imread('image.jpg')
image_rot = cv2.imread('image_rot.jpg')
gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
gray_rot = cv2.cvtColor(image_rot,cv2.COLOR_BGR2GRAY)

sift = cv2.xfeatures2d.SIFT_create()

kp, desc = sift.detectAndCompute(gray,None)
kp_rot, desc_rot = sift.detectAndCompute(gray_rot, None)

bf = cv2.BFMatcher()
matches = bf.knnMatch(desc,desc_rot, k=2)

good = []
for m,n in matches:
    if m.distance < 0.4*n.distance:
    good.append([m])

random.shuffle(good)
# cv2.drawMatchesKnn expects list of lists as matches.
image_match = cv2.drawMatchesKnn(image,kp,image_rot,kp_rot,good[:10],flags=2, outImg=None)

cv2.imwrite('sift_matches.jpg',image_match)

- Speeded up robust features
Fast interest point detection
Distinctive interest point description
Speeded up descriptor matching

- detecting SURF keypoints, For SURF, we will use integral images (like for Haar Cascades) to speed up the keypoint detection step.

Box filters approximate Gaussians and can be calculated very quickly. The following figure shows an approximation of Gaussians as box filters:

    + SURF keypoint descriptor, construct a square region that is rotated and aligned based on the selected orientation

    + orientation assignment, he responses are calculated in a circular neighborhood of radius 6s around the keypoint, where s is the scale of the image (that is, the value of σ). To calculate the Haar wavelet responses, SURF proposes using a wavelet size of 4s after obtaining the wavelet responses and weighing them with a Gaussian kernel (σ = 2.5s) centered

    + descriptor based on Haar wavelet response. This is not required for U-SURF. The size of the window is 20. The following steps are taken to find the descriptors

Split the interest region into 4 x 4 square subregions with 5 x 5 regularly spaced sample points inside.
Calculate Haar wavelet responses dx and dy (dx = Haar wavelet response in x direction; dy = Haar wavelet response in y direction. The filter size used is 2 s).
Weight the response with a Gaussian kernel centered at the interest point.
Sum the response over each subregion for dx and dy separately, to form a feature vector of length 32.
In order to bring in information about the polarity of the intensity changes, extract the sum of the absolute value of the responses, which is a feature vector of length 64.
Normalize the vector to the unit length.

    + example 
import cv2
import random

image = cv2.imread('image.jpg')
image_rot = cv2.imread('image_rot.jpg')
gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
gray_rot = cv2.cvtColor(image_rot,cv2.COLOR_BGR2GRAY)

surf = cv2.xfeatures2d.SURF_create()

kp, desc = surf.detectAndCompute(gray,None)
kp_rot, desc_rot = surf.detectAndCompute(gray_rot, None)

bf = cv2.BFMatcher()
matches = bf.knnMatch(desc,desc_rot, k=2)

good = []
for m,n in matches:
    if m.distance < 0.4*n.distance:
    good.append([m])


random.shuffle(good) cv2.drawMatchesKnn(image,kp,image_rot,kp_rot,good[:10],flags=2, outImg=None)

cv2.imwrite('surf_matches.jpg',image_match)


# Video processing using OpenCV 
- Reading/Writing videos 
    + read 
import cv2

cam = cv2.VideoCapture(0)

while (cam.isOpened()):

         ret, frame = cam.read()
         cv2.imshow('frame',frame)

         if cv2.waitKey(1) & 0xFF == ord('q'):
                  break

cam.release()
cv2.destroyAllWindows()

    + write 
import cv2

cam = cv2.VideoCapture(0)
ret, frame = cam.read()

h, w = frame.shape[:2]
fourcc = cv2.VideoWriter_fourcc(*'DIVX')
video_write = cv2.VideoWriter(saved_out.avi', fourcc, 25.0, (w, h) )

while (cam.isOpened()):

         ret, frame = cam.read()
         video_write.write(frame)
         cv2.imshow('video',frame)

         if cv2.waitKey(1) & 0xFF == ord('q'):
                  break

cam.release()
video_write.release()
cv2.destroyAllWindows()

- convert to grayscale 
import cv2

cam = cv2.VideoCapture(0)

while (cam.isOpened()):

         ret, frame = cam.read()

         gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
         cv2.imshow('gray_frame',gray_frame)
         cv2.imshow('original_frame',frame)

         if cv2.waitKey(1) & 0xFF == ord('q'):
                  break

cam.release()
cv2.destroyAllWindows()

- color tracking 
import cv2
import numpy as np

def detect(img):

         lower_range = np.array([40,150,150], dtype = "uint8")
         upper_range = np.array([70,255,255], dtype = "uint8")

         img = cv2.inRange(img,lower_range,upper_range)
         cv2.imshow("Range",img)

         m=cv2.moments(img)
         if (m["m00"] != 0):
                  x = int(m["m10"]/m["m00"])
                  y = int(m["m01"]/m["m00"])
         else:
                  x = 0
                  y = 0

         return (x, y)


cam = cv2.VideoCapture(0)


# Object tracking 
- trackers available in OpenCV 
KCF,  Kernelized Correlation Filter (KCF) to track an object. 
Lucas Kanade Tracker 
MIL 
BOOSTING 
MEDIANFLOW 
TLD 

- KCF, Given the initial set of points, a tracker tries to calculate the motion of these points by looking at the direction of change in the next frame. In every consecutive frame, we try to look for the same set of points in the neighborhood.

import cv2

tracker = cv2.Tracker_create("KCF")

cam = cv2.VideoCapture(0)
for i in range(5):
    ret, frame = cam.read()

obj = cv2.selectROI("Tracking",frame)

ok = tracker.init(frame, obj)

while True:

    ret, frame = cam.read()

    upd, obj = tracker.update(frame)
    if upd:
        x1 = (int(obj[0]), int(obj[1]))
        x2 = (int(obj[0] + obj[2]), int(obj[1] + obj[3]))
        cv2.rectangle(frame[1], x1, x2, (255,0,0))
    cv2.imshow("Track object", frame)

    k = cv2.waitKey(1) & 0xff
    if k == 27 :
        break

cam.release()
cv2.destroyAllWindows()

    + research paper, The published research paper for Kernelized Correlation Filters is available at https://arxiv.org/pdf/1404.7584.pdf.

- Lucas Kanade Tracker(LK Tracker), LK Tracker, we will select the points to follow by extracting key points from a given image and we will only follow these key points in the given sequence of images. 

import numpy as np
import cv2

cap = cv2.VideoCapture(0)

feature_params = dict( maxCorners = 1000,
                 qualityLevel = 0.3,
                 minDistance = 7,
                 blockSize = 5,
                 useHarrisDetector=1,
                 k=0.04)

lk_params = dict( winSize = (15,15),
 maxLevel = 2)

color = np.random.randint(0,255,(1000,3))

ret, old_frame = cap.read()
old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)

p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)

mask = np.zeros_like(old_frame)

count = 0 #To keep track of how many frames have been read

while(cam.isOpened()):
     ret, frame = cap.read()
     frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
     p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)
    
     good_new = p1[st==1]
     good_old = p0[st==1]
    
     for i,(new,old) in enumerate(zip(good_new,good_old)):
         a,b = new.ravel()
         c,d = old.ravel()
         mask = cv2.line(mask, (a,b),(c,d), color[i].tolist(), 2)
         frame = cv2.circle(frame,(a,b),5,color[i].tolist(),-1)
         img = cv2.add(frame,mask)
         cv2.imshow('frame',img)
         k = cv2.waitKey(30) & 0xff
         if k == 27:
         break
    
     old_gray = frame_gray.copy()
    
     count = count + 1
     if count % 100 == 0:
        p0 = cv2.goodFeaturesToTrack(old_gray, mask = None,    **feature_params)
     else:
        p0 = good_new.reshape(-1,1,2)
cv2.destroyAllWindows()
cap.release()


# Computer vision as a service 
- environment 
virtualenv 
http-server 
flask 
flask-cors, cross-origin requests to the server 

















